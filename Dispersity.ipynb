{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1dcf390",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "9da4d985",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SV(n):\n",
    "    q = n//2\n",
    "    r = n%2\n",
    "    if r == 1:\n",
    "        sv = 3.1416\n",
    "        for i in range(1, q+1):\n",
    "            sv *= (2*i+1)/(2*i)\n",
    "    else:\n",
    "        sv = 2\n",
    "        for i in range(1, q+1):\n",
    "            sv *= 2*i/(2*i-1)\n",
    "    return sv\n",
    "\n",
    "\n",
    "def h(n):\n",
    "    h = np.sqrt((n+1)/(2*n))\n",
    "    return h\n",
    "\n",
    "\n",
    "def LU(m, n):\n",
    "    l = np.power((n+1)*SV(n)/(2*m), 1/n)\n",
    "    u = l/h(n)\n",
    "    return l, u\n",
    "\n",
    "\n",
    "\n",
    "def ad_function(W, theta, target_norm=1):\n",
    "    m = W.shape[0]\n",
    "    WWT = W @ torch.t(W)\n",
    "    norm2 = torch.diagonal(WWT, 0)\n",
    "    with torch.no_grad():\n",
    "        N = (torch.sqrt(norm2[:, None] @ norm2[None, :]) + 1e-8)*1.001\n",
    "    if theta == 1.5708:\n",
    "        M = torch.logical_not(torch.eye(m, dtype=bool).cuda())\n",
    "        tloss = torch.sum(((torch.arccos(WWT[M]) - theta))**2)\n",
    "    else:\n",
    "        WWTN = WWT/N\n",
    "#         WWTNN = WWTN/N\n",
    "        Z = torch.logical_not(torch.eye(m, dtype=bool).cuda())\n",
    "        M1 = (WWTN > np.cos(theta)) * (WWT < 0.99)*Z\n",
    "        M2 = (WWTN < -np.cos(theta)) * (WWT > -0.99)*Z\n",
    "\n",
    "        tloss = torch.sum((torch.arccos(WWTN[M1]) - theta)**2) + \\\n",
    "            torch.sum((torch.arccos(WWTN[M2]) - 3.1416 + theta)**2)   \n",
    "\n",
    "#         tloss = torch.sum((torch.arccos(WWT[M1]) - theta)**2) + \\\n",
    "#             torch.sum((torch.arccos(WWT[M2]) - 3.1416 + theta)**2)\n",
    "\n",
    "#         print(torch.sum(M1), torch.sum(M2), tloss)\n",
    "\n",
    "    nloss = torch.sum((target_norm**2 - norm2)**2)\n",
    "    return nloss, tloss\n",
    "\n",
    "\n",
    "\n",
    "def ADK(weight, theta, double=False):\n",
    "    if double:\n",
    "#         nloss, tloss = ad_function(weight, 1.5708, target_norm=1)\n",
    "#         n2, t2 = ad_function(torch.t(weight), theta, target_norm=0.01)\n",
    "#         nloss += n2\n",
    "#         tloss += t2\n",
    "        nloss, tloss = ad_function(torch.t(weight), theta, target_norm=0.1)\n",
    "    else:\n",
    "        nloss, tloss = ad_function(weight, 1.5708, target_norm=1)\n",
    "    return  nloss, tloss\n",
    "\n",
    "\n",
    "def SO(weight):\n",
    "    if isinstance(weight, tuple):\n",
    "        weight = weight[0]\n",
    "    m = weight.shape[0]\n",
    "    W = weight.view(m, -1)\n",
    "    loss = torch.sum((W @ torch.t(W) - torch.eye(m, dtype=float).cuda())**2)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def DSO(weight):\n",
    "    if isinstance(weight, tuple):\n",
    "        weight = weight[0]\n",
    "    m = weight.shape[0]\n",
    "    W = weight.view(m, -1)\n",
    "    n = W.shape[1]\n",
    "    loss = torch.sum((W @ torch.t(W) - torch.eye(m, dtype=float).cuda())**2) + \\\n",
    "        torch.sum((torch.t(W) @ W - torch.eye(n, dtype=float).cuda())**2) \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8dc55363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def angle_analysis(W):\n",
    "    m = W.shape[0]\n",
    "    WWT = W @ torch.t(W)\n",
    "    norm2 = torch.diagonal(WWT, 0)\n",
    "    N = (torch.sqrt(norm2[:, None] @ norm2[None, :]).detach() + 1e-8)*1.001  \n",
    "    WWTN = WWT/N\n",
    "    \n",
    "    M = torch.logical_not(torch.eye(m))\n",
    "    sp = torch.sort(1 - torch.abs(WWTN[M].view(m, -1)), dim=1)\n",
    "    \n",
    "    theta = torch.arccos(torch.abs(WWTN[torch.arange(m), sp.indices[:, 0]]))\n",
    "    mean = torch.mean(theta)\n",
    "    Max = torch.amax(theta)\n",
    "    Min = torch.amin(theta)\n",
    "\n",
    "    return mean, Min, Max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f21af1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bsize = 512\n",
    "num_worker = 4\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "210b53e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "                root='./DATA/', \n",
    "                transform=transforms.Compose(\n",
    "                    [\n",
    "                    transforms.RandomCrop(32, padding=4),\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    transforms.RandomRotation(15),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
    "                    ]),\n",
    "                train=True)\n",
    "\n",
    "val_dataset = torchvision.datasets.CIFAR10(\n",
    "                root='./DATA/', \n",
    "                transform=transforms.Compose(\n",
    "                    [\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
    "                    ]),\n",
    "                train=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c50e525b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "                    train_dataset, \n",
    "                    batch_size=bsize, \n",
    "                    shuffle=True, \n",
    "                    num_workers=num_worker, \n",
    "                    pin_memory=True, \n",
    "                    )\n",
    "\n",
    "val_loader = DataLoader(\n",
    "                    val_dataset, \n",
    "                    batch_size=bsize, \n",
    "                    shuffle=False, \n",
    "                    num_workers=num_worker, \n",
    "                    pin_memory=True, \n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85e899f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(32*32*3, 1000)\n",
    "        self.layer2 = nn.Linear(1000, 1000)\n",
    "        self.layer3 = nn.Linear(1000, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 32*32*3)\n",
    "        x = self.layer1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.layer3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "981765ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7328738236546221 0.9882068622217131\n"
     ]
    }
   ],
   "source": [
    "l, u = LU(1000, 10)\n",
    "print(l, u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f9c7945",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ad_function_update(W, theta, target_norm=1):\n",
    "    m = W.shape[0]\n",
    "    WWT = W @ torch.t(W)\n",
    "    norm2 = torch.diagonal(WWT, 0)\n",
    "    with torch.no_grad():\n",
    "        N = (torch.sqrt(norm2[:, None] @ norm2[None, :]) + 1e-8)*1.001\n",
    "    if theta == 1.5708:\n",
    "        M = torch.logical_not(torch.eye(m, dtype=bool).cuda())\n",
    "        tloss = torch.sum(((torch.arccos(WWT[M]) - theta))**2)\n",
    "    else:\n",
    "        WWTN = WWT/N\n",
    "        WWTNN = WWTN/N\n",
    "        Z = torch.logical_not(torch.eye(m, dtype=bool).cuda())\n",
    "        M1 = (WWTN > np.cos(theta)) * (WWT < 0.99)*Z\n",
    "        M2 = (WWTN < -np.cos(theta)) * (WWT > -0.99)*Z\n",
    "\n",
    "        tloss = torch.sum((torch.arccos(WWTN[M1]) - theta)**2) + \\\n",
    "            torch.sum((torch.arccos(WWTN[M2]) - 3.1416 + theta)**2)   \n",
    "\n",
    "#         tloss = torch.sum((torch.arccos(WWT[M1]) - theta)**2) + \\\n",
    "#             torch.sum((torch.arccos(WWT[M2]) - 3.1416 + theta)**2)\n",
    "\n",
    "#         print(torch.sum(M1), torch.sum(M2), tloss)\n",
    "\n",
    "#     nloss = torch.sum((target_norm**2 - norm2)**2)\n",
    "    return tloss\n",
    "\n",
    "\n",
    "\n",
    "def ADK_update(weight, theta, lamd, lr, target_norm):\n",
    "#     if isinstance(weight, tuple):\n",
    "#         weight = weight[0]\n",
    "#     m = weight.shape[0]\n",
    "#     W = weight.view(m, -1)\n",
    "    weight1 = weight.clone()\n",
    "    \n",
    "    tloss = ad_function_update(torch.t(weight1), theta, target_norm=0.1)\n",
    "    delW = torch.autograd.grad(tloss, weight1, retain_graph=True)[0]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        newW = weight1 - lr*lamd*(delW/torch.norm(weight1, dim=[1], keepdim=True)).view(weight1.shape)\n",
    "        weight.copy_(newW)\n",
    "    m = weight.shape[0]\n",
    "    WWT = weight @ torch.t(weight)\n",
    "    norm2 = torch.diagonal(WWT, 0)\n",
    "    nloss = torch.sum((target_norm**2 - norm2)**2)\n",
    "    return nloss, tloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "6b27d154",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.39 0.04 1.57 : 0.92 0.19 1.57\n",
      "mean norm:  0.5639896392822266 0.05581842362880707\n",
      "epoch: 0, loss: 2.1267154216766357, accruacy: 0.2675\n",
      "1.37 0.05 1.56 : 0.93 0.20 1.57\n",
      "mean norm:  0.5580971837043762 0.05517022684216499\n",
      "epoch: 1, loss: 2.0740468502044678, accruacy: 0.2921\n",
      "1.36 0.04 1.56 : 0.93 0.04 1.57\n",
      "mean norm:  0.5505606532096863 0.05435553193092346\n",
      "epoch: 2, loss: 2.0093839168548584, accruacy: 0.3098\n",
      "1.35 0.05 1.57 : 0.92 0.04 1.57\n",
      "mean norm:  0.5419009327888489 0.05344051867723465\n",
      "epoch: 3, loss: 1.9270597696304321, accruacy: 0.322\n",
      "1.33 0.05 1.56 : 0.92 0.04 1.57\n",
      "mean norm:  0.5329955220222473 0.052503008395433426\n",
      "epoch: 4, loss: 1.8606401681900024, accruacy: 0.3315\n",
      "1.32 0.05 1.56 : 0.93 0.04 1.57\n",
      "mean norm:  0.5245689749717712 0.05160930007696152\n",
      "epoch: 5, loss: 1.9425522089004517, accruacy: 0.337\n",
      "1.30 0.04 1.54 : 0.93 0.04 1.57\n",
      "mean norm:  0.5177648663520813 0.05087217688560486\n",
      "epoch: 6, loss: 1.8826148509979248, accruacy: 0.345\n",
      "1.30 0.05 1.52 : 0.92 0.04 1.57\n",
      "mean norm:  0.5107728838920593 0.0501200295984745\n",
      "epoch: 7, loss: 1.845780849456787, accruacy: 0.3495\n",
      "1.30 0.05 1.57 : 0.92 0.04 1.57\n",
      "mean norm:  0.5045322775840759 0.04944635182619095\n",
      "epoch: 8, loss: 1.7867861986160278, accruacy: 0.3525\n",
      "1.29 0.05 1.57 : 0.92 0.04 1.57\n",
      "mean norm:  0.4986749291419983 0.04881616309285164\n",
      "epoch: 9, loss: 1.9188642501831055, accruacy: 0.3563\n",
      "1.43 1.30 1.56 : 0.91 0.04 1.57\n",
      "mean norm:  0.4936031401157379 0.048261240124702454\n",
      "epoch: 10, loss: 1.8412595987319946, accruacy: 0.3623\n",
      "1.43 1.28 1.56 : 0.91 0.04 1.57\n",
      "mean norm:  0.48899832367897034 0.04775232449173927\n",
      "epoch: 11, loss: 1.7560018301010132, accruacy: 0.3637\n",
      "1.43 1.26 1.55 : 0.91 0.04 1.57\n",
      "mean norm:  0.4842379093170166 0.0472392812371254\n",
      "epoch: 12, loss: 1.8264049291610718, accruacy: 0.3683\n",
      "1.43 1.24 1.55 : 0.91 0.04 1.57\n",
      "mean norm:  0.4800366461277008 0.04677370563149452\n",
      "epoch: 13, loss: 1.8529404401779175, accruacy: 0.3723\n",
      "1.29 0.05 1.55 : 0.91 0.05 1.57\n",
      "mean norm:  0.4759174883365631 0.046315599232912064\n",
      "epoch: 14, loss: 1.7902119159698486, accruacy: 0.3729\n",
      "1.28 0.05 1.56 : 0.91 0.05 1.57\n",
      "mean norm:  0.471853107213974 0.04586721211671829\n",
      "epoch: 15, loss: 1.8276280164718628, accruacy: 0.3746\n",
      "1.28 0.05 1.56 : 0.91 0.05 1.57\n",
      "mean norm:  0.46857649087905884 0.04549643397331238\n",
      "epoch: 16, loss: 1.7092938423156738, accruacy: 0.3832\n",
      "1.28 0.04 1.57 : 0.90 0.05 1.57\n",
      "mean norm:  0.465162992477417 0.04510953277349472\n",
      "epoch: 17, loss: 1.752698540687561, accruacy: 0.3847\n",
      "1.27 0.04 1.57 : 0.90 0.05 1.57\n",
      "mean norm:  0.4617886543273926 0.044727105647325516\n",
      "epoch: 18, loss: 1.7338289022445679, accruacy: 0.3857\n",
      "1.27 0.05 1.57 : 0.91 0.05 1.57\n",
      "mean norm:  0.4577958583831787 0.04429202526807785\n",
      "epoch: 19, loss: 1.7849452495574951, accruacy: 0.3852\n",
      "1.27 0.05 1.57 : 0.90 0.05 1.57\n",
      "mean norm:  0.4544386565685272 0.04391743615269661\n",
      "epoch: 20, loss: 1.7793810367584229, accruacy: 0.384\n",
      "1.26 0.04 1.56 : 0.90 0.05 1.57\n",
      "mean norm:  0.4517034590244293 0.043602850288152695\n",
      "epoch: 21, loss: 1.7450296878814697, accruacy: 0.3877\n",
      "1.26 0.05 1.56 : 0.90 0.05 1.57\n",
      "mean norm:  0.44831785559654236 0.04322272539138794\n",
      "epoch: 22, loss: 1.6683197021484375, accruacy: 0.3903\n",
      "1.26 0.04 1.56 : 0.90 0.05 1.57\n",
      "mean norm:  0.4453091621398926 0.04289083182811737\n",
      "epoch: 23, loss: 1.630333423614502, accruacy: 0.3911\n",
      "1.26 0.04 1.57 : 0.89 0.26 1.57\n",
      "mean norm:  0.4418811798095703 0.04250975325703621\n",
      "epoch: 24, loss: 1.780522108078003, accruacy: 0.3892\n",
      "1.26 0.05 1.57 : 0.89 0.27 1.57\n",
      "mean norm:  0.43945273756980896 0.04222753271460533\n",
      "epoch: 25, loss: 1.698266625404358, accruacy: 0.3963\n",
      "1.26 0.05 1.57 : 0.89 0.04 1.57\n",
      "mean norm:  0.4362405836582184 0.04186922684311867\n",
      "epoch: 26, loss: 1.7508158683776855, accruacy: 0.3956\n",
      "1.26 0.05 1.57 : 0.88 0.29 1.57\n",
      "mean norm:  0.4333956241607666 0.041546862572431564\n",
      "epoch: 27, loss: 1.6775130033493042, accruacy: 0.3929\n",
      "1.26 0.05 1.56 : 0.88 0.27 1.57\n",
      "mean norm:  0.43089962005615234 0.04125421494245529\n",
      "epoch: 28, loss: 1.6918011903762817, accruacy: 0.3933\n",
      "1.26 0.05 1.57 : 0.88 0.04 1.57\n",
      "mean norm:  0.4286006987094879 0.040978509932756424\n",
      "epoch: 29, loss: 1.6658111810684204, accruacy: 0.3936\n",
      "1.26 0.05 1.56 : 0.88 0.29 1.57\n",
      "mean norm:  0.4266083836555481 0.040738608688116074\n",
      "epoch: 30, loss: 1.6764780282974243, accruacy: 0.3942\n",
      "1.26 0.05 1.57 : 0.89 0.29 1.57\n",
      "mean norm:  0.42414212226867676 0.040456995368003845\n",
      "epoch: 31, loss: 1.825405240058899, accruacy: 0.3995\n",
      "1.26 0.04 1.57 : 0.88 0.04 1.57\n",
      "mean norm:  0.42177149653434753 0.040175553411245346\n",
      "epoch: 32, loss: 1.7194101810455322, accruacy: 0.4007\n",
      "1.10 0.05 1.57 : 0.89 0.04 1.57\n",
      "mean norm:  0.42008301615715027 0.039969027042388916\n",
      "epoch: 33, loss: 1.7612022161483765, accruacy: 0.4036\n",
      "1.10 0.04 1.57 : 0.88 0.04 1.57\n",
      "mean norm:  0.41749629378318787 0.039665598422288895\n",
      "epoch: 34, loss: 1.64097261428833, accruacy: 0.3986\n",
      "1.11 0.04 1.56 : 0.89 0.04 1.57\n",
      "mean norm:  0.41556039452552795 0.03943123668432236\n",
      "epoch: 35, loss: 1.5813242197036743, accruacy: 0.3959\n",
      "1.11 0.04 1.56 : 0.89 0.04 1.57\n",
      "mean norm:  0.41353926062583923 0.03918566182255745\n",
      "epoch: 36, loss: 1.61649751663208, accruacy: 0.3974\n",
      "1.11 0.04 1.56 : 0.89 0.04 1.57\n",
      "mean norm:  0.4116576313972473 0.03895445540547371\n",
      "epoch: 37, loss: 1.7494826316833496, accruacy: 0.3963\n",
      "1.11 0.04 1.56 : 0.89 0.04 1.57\n",
      "mean norm:  0.4098995327949524 0.03872567042708397\n",
      "epoch: 38, loss: 1.5984870195388794, accruacy: 0.4028\n",
      "1.11 0.04 1.56 : 0.88 0.04 1.57\n",
      "mean norm:  0.40828290581703186 0.03852369263768196\n",
      "epoch: 39, loss: 1.6183586120605469, accruacy: 0.4034\n",
      "1.11 0.04 1.56 : 0.89 0.04 1.57\n",
      "mean norm:  0.4074558913707733 0.03839322179555893\n",
      "epoch: 40, loss: 1.5930832624435425, accruacy: 0.4052\n",
      "1.11 0.04 1.55 : 0.88 0.04 1.57\n",
      "mean norm:  0.40604645013809204 0.03821922838687897\n",
      "epoch: 41, loss: 1.6389667987823486, accruacy: 0.4053\n",
      "1.11 0.05 1.56 : 0.89 0.04 1.57\n",
      "mean norm:  0.4051162898540497 0.038080234080553055\n",
      "epoch: 42, loss: 1.6340839862823486, accruacy: 0.405\n",
      "1.11 0.04 1.56 : 0.89 0.04 1.57\n",
      "mean norm:  0.4037491977214813 0.03789248690009117\n",
      "epoch: 43, loss: 1.7542654275894165, accruacy: 0.4016\n",
      "1.11 0.04 1.56 : 0.88 0.04 1.57\n",
      "mean norm:  0.4026266038417816 0.03773987665772438\n",
      "epoch: 44, loss: 1.596482276916504, accruacy: 0.401\n",
      "1.11 0.04 1.56 : 0.88 0.04 1.57\n",
      "mean norm:  0.40111517906188965 0.03753651678562164\n",
      "epoch: 45, loss: 1.6856637001037598, accruacy: 0.4019\n",
      "1.11 0.05 1.56 : 0.88 0.04 1.57\n",
      "mean norm:  0.4002908170223236 0.03740587830543518\n",
      "epoch: 46, loss: 1.6204301118850708, accruacy: 0.4146\n",
      "0.98 0.04 1.57 : 0.88 0.23 1.57\n",
      "mean norm:  0.3991621732711792 0.03725236654281616\n",
      "epoch: 47, loss: 1.6482625007629395, accruacy: 0.408\n",
      "0.98 0.04 1.56 : 0.88 0.04 1.57\n",
      "mean norm:  0.3983045816421509 0.03712688013911247\n",
      "epoch: 48, loss: 1.6591253280639648, accruacy: 0.4057\n",
      "1.11 0.04 1.57 : 0.87 0.04 1.57\n",
      "mean norm:  0.3973518908023834 0.03699706867337227\n",
      "epoch: 49, loss: 1.6317665576934814, accruacy: 0.4085\n"
     ]
    }
   ],
   "source": [
    "# ADK UPDATE double\n",
    "epochs = 50\n",
    "lamd = 0.1\n",
    "lr = 0.01\n",
    "target_norm = 0.1\n",
    "theta = 1.\n",
    "rtn = 0.01\n",
    "\n",
    "model = TestModel().cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "for e in range(epochs):\n",
    "    for x, y in train_loader:\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "        out = model(x)\n",
    "       \n",
    "        W = torch.t(model.layer3.weight)\n",
    "\n",
    "        WWT = W @ torch.t(W)\n",
    "        norm2 = torch.diagonal(WWT, 0)\n",
    "        nloss = torch.sum((target_norm**2 - norm2)**2)        \n",
    "        \n",
    "        loss = criterion(out, y) + lamd*nloss\n",
    "#         loss = nl\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "        weight1 = W.clone()\n",
    "        tloss = ad_function(weight1, theta, target_norm=0.1)\n",
    "        delW = torch.autograd.grad(tloss, weight1)[0]\n",
    "#         print('norm2', norm2)\n",
    "#         print('1', W)\n",
    "        with torch.no_grad():\n",
    "            newW = weight1 - lr*lamd*rtn*(delW*torch.norm(weight1, dim=[1], keepdim=True)**2).view(weight1.shape)\n",
    "            model.layer3.weight.copy_(torch.t(newW))\n",
    "        \n",
    "#         print('2', W)\n",
    "#         print('delW', delW)\n",
    "#         print('norm', torch.norm(weight1, dim=[1], keepdim=True))\n",
    "#         print('delW/norm', (delW*torch.norm(weight1, dim=[1], keepdim=True)**2).view(weight1.shape))\n",
    "        \n",
    "        \n",
    "    W = model.layer3.weight\n",
    "    Mean, Min, Max = angle_analysis(W)\n",
    "    TMean, TMin, TMax = angle_analysis(torch.t(W))\n",
    "    print(f'{Mean.item():3.2f} {Min.item():3.2f} {Max.item():3.2f} : {TMean.item():3.2f} {TMin.item():3.2f} {TMax.item():3.2f}')\n",
    "    print('mean norm: ', torch.mean(torch.norm(W, dim=1)).item(), torch.mean(torch.norm(W, dim=0)).item())\n",
    "    test_num = 0\n",
    "    hit_num = 0\n",
    "    for x, y in val_loader:\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "        test_num += len(y)\n",
    "        out = model(x)\n",
    "        pred = out.argmax(dim=1, keepdim=True)\n",
    "        hit_num += pred.eq(y.view_as(pred)).sum().item()\n",
    "    print(f'epoch: {e}, loss: {loss}, accruacy: {hit_num/test_num}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "0907e253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.23 0.04 1.55 : 0.92 0.04 1.57\n",
      "mean norm:  0.38339099287986755 0.03793233633041382\n",
      "epoch: 0, loss: 2.1696174144744873, accruacy: 0.2496\n",
      "1.35 0.05 1.55 : 0.93 0.25 1.57\n",
      "mean norm:  0.27672240138053894 0.027248242869973183\n",
      "epoch: 1, loss: 2.1227924823760986, accruacy: 0.264\n",
      "1.33 0.04 1.55 : 0.90 0.24 1.57\n",
      "mean norm:  0.20879574120044708 0.02034074440598488\n",
      "epoch: 2, loss: 2.1085901260375977, accruacy: 0.2821\n",
      "1.32 0.05 1.56 : 0.88 0.25 1.57\n",
      "mean norm:  0.16490352153778076 0.01575537584722042\n",
      "epoch: 3, loss: 2.0746278762817383, accruacy: 0.2788\n",
      "1.30 0.05 1.57 : 0.86 0.05 1.57\n",
      "mean norm:  0.1363113820552826 0.01265737134963274\n",
      "epoch: 4, loss: 2.067530632019043, accruacy: 0.2792\n",
      "1.25 0.05 1.52 : 0.86 0.20 1.57\n",
      "mean norm:  0.11836904287338257 0.010626466944813728\n",
      "epoch: 5, loss: 2.1170642375946045, accruacy: 0.2773\n",
      "1.23 0.04 1.55 : 0.83 0.16 1.57\n",
      "mean norm:  0.10809934139251709 0.009459488093852997\n",
      "epoch: 6, loss: 2.061394691467285, accruacy: 0.2857\n",
      "1.19 0.04 1.51 : 0.80 0.06 1.57\n",
      "mean norm:  0.10116074234247208 0.00866929441690445\n",
      "epoch: 7, loss: 2.0716660022735596, accruacy: 0.2993\n",
      "1.16 0.04 1.50 : 0.79 0.17 1.57\n",
      "mean norm:  0.09683182090520859 0.008125330321490765\n",
      "epoch: 8, loss: 2.041917324066162, accruacy: 0.291\n",
      "1.14 0.04 1.49 : 0.78 0.05 1.57\n",
      "mean norm:  0.09408771991729736 0.0077046495862305164\n",
      "epoch: 9, loss: 2.042696714401245, accruacy: 0.2904\n",
      "1.25 0.99 1.50 : 0.78 0.05 1.57\n",
      "mean norm:  0.09283244609832764 0.007496656849980354\n",
      "epoch: 10, loss: 2.0389404296875, accruacy: 0.2972\n",
      "1.23 1.01 1.51 : 0.78 0.05 1.56\n",
      "mean norm:  0.09162761270999908 0.007303301710635424\n",
      "epoch: 11, loss: 2.062291145324707, accruacy: 0.2969\n",
      "1.22 0.96 1.44 : 0.79 0.05 1.57\n",
      "mean norm:  0.09169156849384308 0.0071780807338654995\n",
      "epoch: 12, loss: 2.054745674133301, accruacy: 0.291\n",
      "1.21 0.95 1.44 : 0.79 0.12 1.57\n",
      "mean norm:  0.09271706640720367 0.007183940149843693\n",
      "epoch: 13, loss: 1.9920178651809692, accruacy: 0.2745\n",
      "1.19 0.94 1.39 : 0.78 0.16 1.57\n",
      "mean norm:  0.09252705425024033 0.007050415500998497\n",
      "epoch: 14, loss: 2.0176279544830322, accruacy: 0.2923\n",
      "1.07 0.04 1.31 : 0.79 0.16 1.57\n",
      "mean norm:  0.09242971986532211 0.006969962734729052\n",
      "epoch: 15, loss: 2.0186798572540283, accruacy: 0.2939\n",
      "1.21 0.92 1.47 : 0.78 0.14 1.57\n",
      "mean norm:  0.09320040047168732 0.006886711344122887\n",
      "epoch: 16, loss: 1.9931126832962036, accruacy: 0.2954\n",
      "1.21 0.91 1.50 : 0.78 0.17 1.57\n",
      "mean norm:  0.09337597340345383 0.006789388135075569\n",
      "epoch: 17, loss: 2.003683567047119, accruacy: 0.303\n",
      "1.21 0.90 1.48 : 0.76 0.14 1.57\n",
      "mean norm:  0.09529431909322739 0.006880393251776695\n",
      "epoch: 18, loss: 1.9387433528900146, accruacy: 0.3079\n",
      "0.96 0.04 1.31 : 0.78 0.15 1.57\n",
      "mean norm:  0.09597375988960266 0.006819938775151968\n",
      "epoch: 19, loss: 2.0136826038360596, accruacy: 0.2929\n",
      "0.96 0.05 1.34 : 0.78 0.05 1.57\n",
      "mean norm:  0.09783712774515152 0.0068699815310537815\n",
      "epoch: 20, loss: 2.0163161754608154, accruacy: 0.3116\n",
      "0.84 0.04 1.37 : 0.77 0.05 1.57\n",
      "mean norm:  0.09851717948913574 0.006755226757377386\n",
      "epoch: 21, loss: 2.0527021884918213, accruacy: 0.2826\n",
      "0.97 0.04 1.45 : 0.77 0.14 1.57\n",
      "mean norm:  0.10143665224313736 0.006956889294087887\n",
      "epoch: 22, loss: 1.917987585067749, accruacy: 0.2896\n",
      "0.98 0.04 1.47 : 0.78 0.11 1.57\n",
      "mean norm:  0.10146605968475342 0.006767577491700649\n",
      "epoch: 23, loss: 2.030951976776123, accruacy: 0.2879\n",
      "0.99 0.04 1.54 : 0.77 0.15 1.57\n",
      "mean norm:  0.10279128700494766 0.006761972326785326\n",
      "epoch: 24, loss: 2.010582208633423, accruacy: 0.3155\n",
      "0.98 0.04 1.50 : 0.77 0.05 1.57\n",
      "mean norm:  0.10495870560407639 0.006782569456845522\n",
      "epoch: 25, loss: 1.9661091566085815, accruacy: 0.3082\n",
      "1.08 0.04 1.53 : 0.76 0.07 1.57\n",
      "mean norm:  0.10606729984283447 0.006763790268450975\n",
      "epoch: 26, loss: 1.9292457103729248, accruacy: 0.3159\n",
      "0.98 0.04 1.51 : 0.77 0.05 1.57\n",
      "mean norm:  0.10758624225854874 0.006674741394817829\n",
      "epoch: 27, loss: 2.0342888832092285, accruacy: 0.3216\n",
      "1.06 0.04 1.48 : 0.76 0.15 1.57\n",
      "mean norm:  0.10900642722845078 0.006770837586373091\n",
      "epoch: 28, loss: 1.9664978981018066, accruacy: 0.3163\n",
      "1.06 0.05 1.49 : 0.75 0.15 1.57\n",
      "mean norm:  0.11024954169988632 0.006772109307348728\n",
      "epoch: 29, loss: 1.994352102279663, accruacy: 0.3157\n",
      "1.06 0.04 1.49 : 0.76 0.05 1.57\n",
      "mean norm:  0.11073809117078781 0.006625383626669645\n",
      "epoch: 30, loss: 2.001584529876709, accruacy: 0.3251\n",
      "1.06 0.04 1.50 : 0.76 0.16 1.56\n",
      "mean norm:  0.11406929790973663 0.00684757623821497\n",
      "epoch: 31, loss: 1.9100053310394287, accruacy: 0.324\n",
      "1.06 0.05 1.48 : 0.76 0.05 1.56\n",
      "mean norm:  0.11527309566736221 0.0068095363676548\n",
      "epoch: 32, loss: 1.9896233081817627, accruacy: 0.3112\n",
      "1.04 0.04 1.47 : 0.76 0.05 1.57\n",
      "mean norm:  0.11702342331409454 0.006734445691108704\n",
      "epoch: 33, loss: 1.9179701805114746, accruacy: 0.3106\n",
      "1.05 0.04 1.49 : 0.76 0.05 1.57\n",
      "mean norm:  0.11925330013036728 0.006883142050355673\n",
      "epoch: 34, loss: 1.917545199394226, accruacy: 0.3275\n",
      "1.05 0.05 1.50 : 0.74 0.05 1.57\n",
      "mean norm:  0.12117775529623032 0.006898300722241402\n",
      "epoch: 35, loss: 1.9627981185913086, accruacy: 0.3281\n",
      "1.04 0.04 1.50 : 0.77 0.05 1.57\n",
      "mean norm:  0.1223100945353508 0.006784630939364433\n",
      "epoch: 36, loss: 1.92221200466156, accruacy: 0.3125\n",
      "1.05 0.05 1.51 : 0.75 0.05 1.57\n",
      "mean norm:  0.12410487234592438 0.006827336736023426\n",
      "epoch: 37, loss: 1.9417694807052612, accruacy: 0.3315\n",
      "1.04 0.04 1.51 : 0.76 0.05 1.57\n",
      "mean norm:  0.1251133531332016 0.006621583830565214\n",
      "epoch: 38, loss: 1.9716461896896362, accruacy: 0.3406\n",
      "1.03 0.04 1.48 : 0.76 0.05 1.56\n",
      "mean norm:  0.12790139019489288 0.006852563004940748\n",
      "epoch: 39, loss: 1.917502522468567, accruacy: 0.3114\n",
      "0.91 0.04 1.50 : 0.75 0.05 1.57\n",
      "mean norm:  0.13014303147792816 0.00682080676779151\n",
      "epoch: 40, loss: 1.8693910837173462, accruacy: 0.3365\n",
      "0.90 0.04 1.52 : 0.75 0.05 1.57\n",
      "mean norm:  0.1313655972480774 0.00661051319912076\n",
      "epoch: 41, loss: 1.9616796970367432, accruacy: 0.3342\n",
      "0.91 0.05 1.50 : 0.74 0.05 1.57\n",
      "mean norm:  0.1347411870956421 0.006833625957369804\n",
      "epoch: 42, loss: 1.904789686203003, accruacy: 0.3418\n",
      "0.91 0.04 1.55 : 0.73 0.05 1.57\n",
      "mean norm:  0.13611117005348206 0.006698716897517443\n",
      "epoch: 43, loss: 1.8900036811828613, accruacy: 0.3061\n",
      "0.91 0.05 1.52 : 0.74 0.05 1.56\n",
      "mean norm:  0.13865874707698822 0.0067266495898365974\n",
      "epoch: 44, loss: 1.8269582986831665, accruacy: 0.3347\n",
      "0.89 0.04 1.50 : 0.71 0.06 1.57\n",
      "mean norm:  0.13993650674819946 0.006583607755601406\n",
      "epoch: 45, loss: 1.9238165616989136, accruacy: 0.3329\n",
      "0.89 0.04 1.51 : 0.74 0.06 1.57\n",
      "mean norm:  0.1426490843296051 0.0066819204948842525\n",
      "epoch: 46, loss: 1.8912690877914429, accruacy: 0.3354\n",
      "0.89 0.04 1.49 : 0.74 0.07 1.57\n",
      "mean norm:  0.14399759471416473 0.006613252218812704\n",
      "epoch: 47, loss: 1.8791160583496094, accruacy: 0.3464\n",
      "0.89 0.04 1.47 : 0.72 0.08 1.57\n",
      "mean norm:  0.1455119401216507 0.006743103265762329\n",
      "epoch: 48, loss: 1.9517056941986084, accruacy: 0.3203\n",
      "0.89 0.04 1.56 : 0.74 0.06 1.57\n",
      "mean norm:  0.14715932309627533 0.00664921011775732\n",
      "epoch: 49, loss: 1.9263848066329956, accruacy: 0.3247\n"
     ]
    }
   ],
   "source": [
    "# ADK UPDATE double\n",
    "epochs = 50\n",
    "lamd = 0.1\n",
    "lr = 0.01\n",
    "target_norm = 0.1\n",
    "theta = 1.\n",
    "rtn = 0.1\n",
    "\n",
    "model = TestModel().cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "for e in range(epochs):\n",
    "    for x, y in train_loader:\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "        out = model(x)\n",
    "       \n",
    "        W = torch.t(model.layer3.weight)\n",
    "\n",
    "        WWT = W @ torch.t(W)\n",
    "        norm2 = torch.diagonal(WWT, 0)\n",
    "        nloss = torch.sum((target_norm**2 - norm2)**2)        \n",
    "        \n",
    "        loss = criterion(out, y) + lamd*nloss\n",
    "#         loss = nl\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "        weight1 = W.clone()\n",
    "        tloss = ad_function(weight1, theta, target_norm=0.1)\n",
    "        delW = torch.autograd.grad(tloss, weight1)[0]\n",
    "#         print('norm2', norm2)\n",
    "#         print('1', W)\n",
    "        with torch.no_grad():\n",
    "            newW = weight1 - lr*lamd*rtn*(delW*torch.norm(weight1, dim=[1], keepdim=True)**2).view(weight1.shape)\n",
    "            model.layer3.weight.copy_(torch.t(newW))\n",
    "        \n",
    "#         print('2', W)\n",
    "#         print('delW', delW)\n",
    "#         print('norm', torch.norm(weight1, dim=[1], keepdim=True))\n",
    "#         print('delW/norm', (delW*torch.norm(weight1, dim=[1], keepdim=True)**2).view(weight1.shape))\n",
    "        \n",
    "        \n",
    "    W = model.layer3.weight\n",
    "    Mean, Min, Max = angle_analysis(W)\n",
    "    TMean, TMin, TMax = angle_analysis(torch.t(W))\n",
    "    print(f'{Mean.item():3.2f} {Min.item():3.2f} {Max.item():3.2f} : {TMean.item():3.2f} {TMin.item():3.2f} {TMax.item():3.2f}')\n",
    "    print('mean norm: ', torch.mean(torch.norm(W, dim=1)).item(), torch.mean(torch.norm(W, dim=0)).item())\n",
    "    test_num = 0\n",
    "    hit_num = 0\n",
    "    for x, y in val_loader:\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "        test_num += len(y)\n",
    "        out = model(x)\n",
    "        pred = out.argmax(dim=1, keepdim=True)\n",
    "        hit_num += pred.eq(y.view_as(pred)).sum().item()\n",
    "    print(f'epoch: {e}, loss: {loss}, accruacy: {hit_num/test_num}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "d4be3038",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.35 0.05 1.57 : 0.91 0.29 1.57\n",
      "mean norm:  0.018144696950912476 0.001708401134237647\n",
      "epoch: 0, loss: 2.3048758506774902, accruacy: 0.1842\n",
      "1.36 1.26 1.57 : 0.93 0.19 1.57\n",
      "mean norm:  0.006043476518243551 0.0005289650289341807\n",
      "epoch: 1, loss: 2.309612274169922, accruacy: 0.177\n",
      "1.34 1.18 1.51 : 0.92 0.34 1.57\n",
      "mean norm:  0.005311189219355583 0.00046188506530597806\n",
      "epoch: 2, loss: 2.309718370437622, accruacy: 0.18\n",
      "1.34 1.14 1.53 : 0.91 0.33 1.57\n",
      "mean norm:  0.0054674046114087105 0.0004710395587608218\n",
      "epoch: 3, loss: 2.3085503578186035, accruacy: 0.1576\n",
      "1.36 1.16 1.56 : 0.92 0.33 1.57\n",
      "mean norm:  0.005427140276879072 0.00045919031254015863\n",
      "epoch: 4, loss: 2.308847665786743, accruacy: 0.1825\n",
      "1.27 0.05 1.56 : 0.92 0.16 1.57\n",
      "mean norm:  0.005384047981351614 0.0004577586951199919\n",
      "epoch: 5, loss: 2.3108737468719482, accruacy: 0.1856\n",
      "1.24 0.05 1.53 : 0.93 0.28 1.57\n",
      "mean norm:  0.005299943964928389 0.0004467568069230765\n",
      "epoch: 6, loss: 2.310016632080078, accruacy: 0.1374\n",
      "1.32 1.11 1.53 : 0.92 0.32 1.57\n",
      "mean norm:  0.00542491814121604 0.0004521613009274006\n",
      "epoch: 7, loss: 2.309026002883911, accruacy: 0.2096\n",
      "1.18 0.06 1.51 : 0.92 0.31 1.57\n",
      "mean norm:  0.005517587997019291 0.0004547367279883474\n",
      "epoch: 8, loss: 2.3092575073242188, accruacy: 0.194\n",
      "1.30 1.19 1.51 : 0.91 0.32 1.57\n",
      "mean norm:  0.005271189846098423 0.00043548023677431047\n",
      "epoch: 9, loss: 2.310142993927002, accruacy: 0.1853\n",
      "1.04 0.05 1.51 : 0.90 0.21 1.57\n",
      "mean norm:  0.005488019436597824 0.0004582130059134215\n",
      "epoch: 10, loss: 2.309835195541382, accruacy: 0.2174\n",
      "1.16 0.05 1.55 : 0.90 0.22 1.57\n",
      "mean norm:  0.00542146060615778 0.00045202692854218185\n",
      "epoch: 11, loss: 2.307899236679077, accruacy: 0.1734\n",
      "1.15 0.05 1.57 : 0.92 0.25 1.57\n",
      "mean norm:  0.005173617508262396 0.0004316380072850734\n",
      "epoch: 12, loss: 2.3088326454162598, accruacy: 0.1917\n",
      "1.17 0.05 1.53 : 0.91 0.30 1.57\n",
      "mean norm:  0.005149421747773886 0.00043283935519866645\n",
      "epoch: 13, loss: 2.3082528114318848, accruacy: 0.1773\n",
      "1.00 0.06 1.47 : 0.93 0.33 1.57\n",
      "mean norm:  0.005152386147528887 0.00043311755871400237\n",
      "epoch: 14, loss: 2.309340715408325, accruacy: 0.193\n",
      "1.20 0.05 1.52 : 0.91 0.17 1.57\n",
      "mean norm:  0.005185218993574381 0.00043779576662927866\n",
      "epoch: 15, loss: 2.3089680671691895, accruacy: 0.2019\n",
      "1.36 1.12 1.54 : 0.91 0.19 1.57\n",
      "mean norm:  0.005111300852149725 0.0004293273377697915\n",
      "epoch: 16, loss: 2.3090875148773193, accruacy: 0.2091\n",
      "1.18 0.06 1.55 : 0.91 0.16 1.57\n",
      "mean norm:  0.005083342548459768 0.0004269227501936257\n",
      "epoch: 17, loss: 2.309291362762451, accruacy: 0.238\n",
      "1.28 1.03 1.50 : 0.91 0.33 1.57\n",
      "mean norm:  0.0051334635354578495 0.0004395705182105303\n",
      "epoch: 18, loss: 2.3091275691986084, accruacy: 0.1868\n",
      "1.27 1.08 1.46 : 0.90 0.13 1.57\n",
      "mean norm:  0.004985780455172062 0.00041534367483109236\n",
      "epoch: 19, loss: 2.3087806701660156, accruacy: 0.2003\n",
      "1.15 0.06 1.55 : 0.91 0.30 1.57\n",
      "mean norm:  0.005235159303992987 0.0004365165950730443\n",
      "epoch: 20, loss: 2.3076870441436768, accruacy: 0.1897\n",
      "1.18 0.06 1.54 : 0.93 0.30 1.57\n",
      "mean norm:  0.005070657003670931 0.000415011279983446\n",
      "epoch: 21, loss: 2.308391809463501, accruacy: 0.207\n",
      "1.18 0.06 1.49 : 0.90 0.25 1.57\n",
      "mean norm:  0.005096992943435907 0.00042749763815663755\n",
      "epoch: 22, loss: 2.3079917430877686, accruacy: 0.225\n",
      "1.10 0.06 1.50 : 0.92 0.26 1.57\n",
      "mean norm:  0.005135671701282263 0.0004231595085002482\n",
      "epoch: 23, loss: 2.308791160583496, accruacy: 0.2245\n",
      "1.16 0.05 1.54 : 0.92 0.29 1.57\n",
      "mean norm:  0.00534912059083581 0.0004351522075012326\n",
      "epoch: 24, loss: 2.3076837062835693, accruacy: 0.1857\n",
      "1.17 0.05 1.55 : 0.89 0.16 1.57\n",
      "mean norm:  0.005216978024691343 0.0004291635996196419\n",
      "epoch: 25, loss: 2.3082025051116943, accruacy: 0.1909\n",
      "1.19 0.05 1.55 : 0.90 0.20 1.57\n",
      "mean norm:  0.005360520910471678 0.0004406018415465951\n",
      "epoch: 26, loss: 2.3094117641448975, accruacy: 0.1819\n",
      "1.06 0.05 1.37 : 0.90 0.26 1.57\n",
      "mean norm:  0.00519233662635088 0.00042009042226709425\n",
      "epoch: 27, loss: 2.308037757873535, accruacy: 0.2082\n",
      "1.06 0.05 1.45 : 0.94 0.33 1.57\n",
      "mean norm:  0.005152840167284012 0.00040664977859705687\n",
      "epoch: 28, loss: 2.307870388031006, accruacy: 0.2187\n",
      "1.19 0.99 1.53 : 0.94 0.14 1.57\n",
      "mean norm:  0.005016918759793043 0.0003954908752348274\n",
      "epoch: 29, loss: 2.3067002296447754, accruacy: 0.1573\n",
      "0.93 0.05 1.31 : 0.92 0.31 1.57\n",
      "mean norm:  0.0051195742562413216 0.00041292785317637026\n",
      "epoch: 30, loss: 2.308458089828491, accruacy: 0.2136\n",
      "1.13 0.05 1.55 : 0.93 0.23 1.57\n",
      "mean norm:  0.005083964206278324 0.00039629937964491546\n",
      "epoch: 31, loss: 2.307086944580078, accruacy: 0.2247\n",
      "1.15 0.05 1.53 : 0.89 0.25 1.57\n",
      "mean norm:  0.004969493485987186 0.00040569750126451254\n",
      "epoch: 32, loss: 2.3094284534454346, accruacy: 0.2296\n",
      "0.95 0.05 1.50 : 0.92 0.30 1.57\n",
      "mean norm:  0.005134464241564274 0.000410694454330951\n",
      "epoch: 33, loss: 2.3083927631378174, accruacy: 0.2362\n",
      "0.83 0.05 1.37 : 0.91 0.27 1.57\n",
      "mean norm:  0.005429437384009361 0.00043884883052669466\n",
      "epoch: 34, loss: 2.306065559387207, accruacy: 0.2132\n",
      "0.96 0.05 1.37 : 0.94 0.28 1.57\n",
      "mean norm:  0.004952085670083761 0.0003796149103436619\n",
      "epoch: 35, loss: 2.3060402870178223, accruacy: 0.2177\n",
      "1.14 0.05 1.56 : 0.91 0.30 1.57\n",
      "mean norm:  0.005224739201366901 0.00042052046046592295\n",
      "epoch: 36, loss: 2.3082520961761475, accruacy: 0.2057\n",
      "1.26 0.99 1.54 : 0.91 0.25 1.57\n",
      "mean norm:  0.0051442701369524 0.0004129578301217407\n",
      "epoch: 37, loss: 2.3075640201568604, accruacy: 0.1902\n",
      "1.15 0.05 1.55 : 0.94 0.30 1.57\n",
      "mean norm:  0.00510504050180316 0.00040644590626470745\n",
      "epoch: 38, loss: 2.3074111938476562, accruacy: 0.2181\n",
      "1.13 0.06 1.53 : 0.90 0.25 1.57\n",
      "mean norm:  0.0051345485262572765 0.0004215468361508101\n",
      "epoch: 39, loss: 2.307931661605835, accruacy: 0.1865\n",
      "1.12 0.05 1.52 : 0.93 0.32 1.57\n",
      "mean norm:  0.004988037049770355 0.0004001060442533344\n",
      "epoch: 40, loss: 2.3079514503479004, accruacy: 0.1908\n",
      "1.13 0.05 1.55 : 0.94 0.25 1.57\n",
      "mean norm:  0.0047686598263680935 0.00037320295814424753\n",
      "epoch: 41, loss: 2.306649923324585, accruacy: 0.1852\n",
      "1.16 0.06 1.53 : 0.95 0.35 1.57\n",
      "mean norm:  0.004916910082101822 0.00039010847103782\n",
      "epoch: 42, loss: 2.307953119277954, accruacy: 0.1703\n",
      "1.15 0.06 1.50 : 0.96 0.25 1.57\n",
      "mean norm:  0.004907685797661543 0.0003873483801726252\n",
      "epoch: 43, loss: 2.307870626449585, accruacy: 0.197\n",
      "0.96 0.05 1.38 : 0.94 0.31 1.57\n",
      "mean norm:  0.004783406388014555 0.000377098738681525\n",
      "epoch: 44, loss: 2.3063626289367676, accruacy: 0.1286\n",
      "1.07 0.05 1.44 : 0.94 0.30 1.57\n",
      "mean norm:  0.004820775240659714 0.00038380103069357574\n",
      "epoch: 45, loss: 2.3070642948150635, accruacy: 0.1556\n",
      "1.18 0.06 1.56 : 0.93 0.30 1.57\n",
      "mean norm:  0.0049831257201731205 0.0004023767833132297\n",
      "epoch: 46, loss: 2.3066353797912598, accruacy: 0.1659\n",
      "0.97 0.05 1.43 : 0.91 0.22 1.57\n",
      "mean norm:  0.004577599465847015 0.00036674595321528614\n",
      "epoch: 47, loss: 2.3091912269592285, accruacy: 0.1939\n",
      "1.14 0.05 1.46 : 0.92 0.30 1.57\n",
      "mean norm:  0.004900399129837751 0.00038659575511701405\n",
      "epoch: 48, loss: 2.307227611541748, accruacy: 0.1685\n",
      "1.20 1.03 1.45 : 0.90 0.26 1.57\n",
      "mean norm:  0.005135257728397846 0.00042985985055565834\n",
      "epoch: 49, loss: 2.3080437183380127, accruacy: 0.1925\n"
     ]
    }
   ],
   "source": [
    "# ADK UPDATE double\n",
    "epochs = 50\n",
    "lamd = 0.1\n",
    "lr = 0.01\n",
    "target_norm = 0.1\n",
    "theta = 1.\n",
    "rtn = 1\n",
    "\n",
    "model = TestModel().cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "for e in range(epochs):\n",
    "    for x, y in train_loader:\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "        out = model(x)\n",
    "       \n",
    "        W = torch.t(model.layer3.weight)\n",
    "\n",
    "        WWT = W @ torch.t(W)\n",
    "        norm2 = torch.diagonal(WWT, 0)\n",
    "        nloss = torch.sum((target_norm**2 - norm2)**2)        \n",
    "        \n",
    "        loss = criterion(out, y) + lamd*nloss\n",
    "#         loss = nl\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "        weight1 = W.clone()\n",
    "        tloss = ad_function(weight1, theta, target_norm=0.1)\n",
    "        delW = torch.autograd.grad(tloss, weight1)[0]\n",
    "#         print('norm2', norm2)\n",
    "#         print('1', W)\n",
    "        with torch.no_grad():\n",
    "            newW = weight1 - lr*lamd*rtn*(delW*torch.norm(weight1, dim=[1], keepdim=True)**2).view(weight1.shape)\n",
    "            model.layer3.weight.copy_(torch.t(newW))\n",
    "        \n",
    "#         print('2', W)\n",
    "#         print('delW', delW)\n",
    "#         print('norm', torch.norm(weight1, dim=[1], keepdim=True))\n",
    "#         print('delW/norm', (delW*torch.norm(weight1, dim=[1], keepdim=True)**2).view(weight1.shape))\n",
    "        \n",
    "        \n",
    "    W = model.layer3.weight\n",
    "    Mean, Min, Max = angle_analysis(W)\n",
    "    TMean, TMin, TMax = angle_analysis(torch.t(W))\n",
    "    print(f'{Mean.item():3.2f} {Min.item():3.2f} {Max.item():3.2f} : {TMean.item():3.2f} {TMin.item():3.2f} {TMax.item():3.2f}')\n",
    "    print('mean norm: ', torch.mean(torch.norm(W, dim=1)).item(), torch.mean(torch.norm(W, dim=0)).item())\n",
    "    test_num = 0\n",
    "    hit_num = 0\n",
    "    for x, y in val_loader:\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "        test_num += len(y)\n",
    "        out = model(x)\n",
    "        pred = out.argmax(dim=1, keepdim=True)\n",
    "        hit_num += pred.eq(y.view_as(pred)).sum().item()\n",
    "    print(f'epoch: {e}, loss: {loss}, accruacy: {hit_num/test_num}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "1f3e80c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.54 1.50 1.57 : 0.91 0.04 1.57\n",
      "mean norm:  0.3846983015537262 0.0380522757768631\n",
      "epoch: 0, loss: 2.168888807296753, accruacy: 0.2398\n",
      "1.36 0.05 1.54 : 0.92 0.04 1.57\n",
      "mean norm:  0.27690139412879944 0.027287045493721962\n",
      "epoch: 1, loss: 2.1098759174346924, accruacy: 0.2542\n",
      "1.33 0.05 1.54 : 0.93 0.30 1.57\n",
      "mean norm:  0.20760364830493927 0.02031354233622551\n",
      "epoch: 2, loss: 2.10550856590271, accruacy: 0.2798\n",
      "1.32 0.04 1.56 : 0.90 0.22 1.57\n",
      "mean norm:  0.1627831608057022 0.015678657218813896\n",
      "epoch: 3, loss: 2.053070306777954, accruacy: 0.2722\n",
      "1.29 0.05 1.55 : 0.85 0.15 1.57\n",
      "mean norm:  0.1341269612312317 0.012579708360135555\n",
      "epoch: 4, loss: 2.039233922958374, accruacy: 0.2715\n",
      "1.25 0.05 1.55 : 0.83 0.05 1.57\n",
      "mean norm:  0.11604940891265869 0.01051049679517746\n",
      "epoch: 5, loss: 2.045306921005249, accruacy: 0.2719\n",
      "1.21 0.05 1.54 : 0.82 0.05 1.57\n",
      "mean norm:  0.1057608500123024 0.009276111610233784\n",
      "epoch: 6, loss: 2.0622289180755615, accruacy: 0.2731\n",
      "1.18 0.04 1.49 : 0.82 0.14 1.57\n",
      "mean norm:  0.09955152869224548 0.008467313833534718\n",
      "epoch: 7, loss: 2.069767951965332, accruacy: 0.2741\n",
      "1.28 1.10 1.47 : 0.80 0.05 1.57\n",
      "mean norm:  0.09573177248239517 0.007960022427141666\n",
      "epoch: 8, loss: 2.0214638710021973, accruacy: 0.2908\n",
      "1.14 0.04 1.47 : 0.81 0.11 1.57\n",
      "mean norm:  0.09296295791864395 0.007549374829977751\n",
      "epoch: 9, loss: 2.042397975921631, accruacy: 0.2885\n",
      "1.23 1.02 1.47 : 0.79 0.05 1.57\n",
      "mean norm:  0.09194649755954742 0.0073384507559239864\n",
      "epoch: 10, loss: 2.013529062271118, accruacy: 0.2978\n",
      "1.10 0.05 1.43 : 0.77 0.04 1.56\n",
      "mean norm:  0.0913304015994072 0.007142860908061266\n",
      "epoch: 11, loss: 2.0279996395111084, accruacy: 0.2814\n",
      "1.09 0.05 1.44 : 0.78 0.04 1.57\n",
      "mean norm:  0.09144455939531326 0.00707338098436594\n",
      "epoch: 12, loss: 1.9893226623535156, accruacy: 0.299\n",
      "1.08 0.05 1.43 : 0.77 0.04 1.57\n",
      "mean norm:  0.09144638478755951 0.006921201013028622\n",
      "epoch: 13, loss: 1.9791622161865234, accruacy: 0.2752\n",
      "1.20 0.95 1.42 : 0.77 0.04 1.57\n",
      "mean norm:  0.0922316238284111 0.00687586423009634\n",
      "epoch: 14, loss: 2.0167741775512695, accruacy: 0.2944\n",
      "1.08 0.05 1.47 : 0.79 0.04 1.57\n",
      "mean norm:  0.09293874353170395 0.006793733220547438\n",
      "epoch: 15, loss: 1.9951145648956299, accruacy: 0.2982\n",
      "1.06 0.05 1.42 : 0.77 0.14 1.57\n",
      "mean norm:  0.09441816806793213 0.006875462830066681\n",
      "epoch: 16, loss: 2.0048506259918213, accruacy: 0.303\n",
      "1.05 0.05 1.38 : 0.77 0.04 1.57\n",
      "mean norm:  0.09545213729143143 0.00682219211012125\n",
      "epoch: 17, loss: 1.9348530769348145, accruacy: 0.2885\n",
      "1.05 0.04 1.42 : 0.76 0.07 1.57\n",
      "mean norm:  0.0956430584192276 0.0067557692527771\n",
      "epoch: 18, loss: 1.9887747764587402, accruacy: 0.2926\n",
      "1.05 0.05 1.47 : 0.76 0.04 1.57\n",
      "mean norm:  0.0965661108493805 0.006755238398909569\n",
      "epoch: 19, loss: 1.9918293952941895, accruacy: 0.3102\n",
      "1.04 0.05 1.40 : 0.78 0.05 1.57\n",
      "mean norm:  0.09780054539442062 0.006773877423256636\n",
      "epoch: 20, loss: 1.9845378398895264, accruacy: 0.3109\n",
      "1.03 0.05 1.37 : 0.77 0.06 1.56\n",
      "mean norm:  0.09792550653219223 0.00671659130603075\n",
      "epoch: 21, loss: 1.9732720851898193, accruacy: 0.294\n",
      "1.03 0.04 1.36 : 0.75 0.16 1.57\n",
      "mean norm:  0.09880530089139938 0.006660079583525658\n",
      "epoch: 22, loss: 1.971916913986206, accruacy: 0.2831\n",
      "1.03 0.04 1.35 : 0.77 0.18 1.57\n",
      "mean norm:  0.10027744621038437 0.006674764212220907\n",
      "epoch: 23, loss: 1.9387340545654297, accruacy: 0.3102\n",
      "1.03 0.05 1.37 : 0.77 0.12 1.57\n",
      "mean norm:  0.10078095644712448 0.006650240160524845\n",
      "epoch: 24, loss: 1.9288935661315918, accruacy: 0.3085\n",
      "1.04 0.05 1.41 : 0.76 0.11 1.57\n",
      "mean norm:  0.10218725353479385 0.006688187830150127\n",
      "epoch: 25, loss: 1.935315728187561, accruacy: 0.2791\n",
      "1.03 0.04 1.38 : 0.78 0.06 1.57\n",
      "mean norm:  0.10379743576049805 0.006712300702929497\n",
      "epoch: 26, loss: 1.9224731922149658, accruacy: 0.29\n",
      "0.93 0.05 1.36 : 0.76 0.11 1.57\n",
      "mean norm:  0.10437498241662979 0.006589000578969717\n",
      "epoch: 27, loss: 1.9833132028579712, accruacy: 0.3178\n",
      "0.93 0.05 1.48 : 0.77 0.07 1.57\n",
      "mean norm:  0.10594196617603302 0.0065493457950651646\n",
      "epoch: 28, loss: 1.9388507604599, accruacy: 0.3122\n",
      "1.06 0.04 1.48 : 0.76 0.16 1.57\n",
      "mean norm:  0.1072746068239212 0.006625323556363583\n",
      "epoch: 29, loss: 1.9000513553619385, accruacy: 0.3216\n",
      "0.92 0.04 1.38 : 0.76 0.05 1.57\n",
      "mean norm:  0.1086239367723465 0.006581940688192844\n",
      "epoch: 30, loss: 1.9433928728103638, accruacy: 0.3048\n",
      "1.07 0.05 1.51 : 0.75 0.11 1.57\n",
      "mean norm:  0.11039680242538452 0.0065910001285374165\n",
      "epoch: 31, loss: 1.955743670463562, accruacy: 0.3378\n",
      "0.95 0.04 1.56 : 0.75 0.15 1.57\n",
      "mean norm:  0.11179828643798828 0.00655424315482378\n",
      "epoch: 32, loss: 1.9379960298538208, accruacy: 0.3039\n",
      "0.94 0.04 1.52 : 0.77 0.16 1.57\n",
      "mean norm:  0.114266537129879 0.006509437225759029\n",
      "epoch: 33, loss: 1.956725835800171, accruacy: 0.318\n",
      "0.94 0.05 1.57 : 0.76 0.06 1.57\n",
      "mean norm:  0.11619886010885239 0.006543594412505627\n",
      "epoch: 34, loss: 1.983883023262024, accruacy: 0.3279\n",
      "0.94 0.04 1.54 : 0.77 0.05 1.57\n",
      "mean norm:  0.11750870198011398 0.006468500476330519\n",
      "epoch: 35, loss: 1.9376745223999023, accruacy: 0.3215\n",
      "1.05 0.05 1.54 : 0.75 0.05 1.57\n",
      "mean norm:  0.11968648433685303 0.006673896685242653\n",
      "epoch: 36, loss: 1.9063360691070557, accruacy: 0.3196\n",
      "0.95 0.04 1.53 : 0.73 0.14 1.57\n",
      "mean norm:  0.12080933898687363 0.006505874916911125\n",
      "epoch: 37, loss: 1.874170184135437, accruacy: 0.3235\n",
      "0.96 0.04 1.56 : 0.75 0.05 1.57\n",
      "mean norm:  0.12288255989551544 0.006545515730977058\n",
      "epoch: 38, loss: 1.9174690246582031, accruacy: 0.3132\n",
      "1.07 0.04 1.55 : 0.74 0.05 1.57\n",
      "mean norm:  0.12417798489332199 0.006504414137452841\n",
      "epoch: 39, loss: 1.9132986068725586, accruacy: 0.3252\n",
      "1.07 0.05 1.56 : 0.74 0.05 1.57\n",
      "mean norm:  0.1271086037158966 0.006455302704125643\n",
      "epoch: 40, loss: 1.8779703378677368, accruacy: 0.3282\n",
      "1.19 0.91 1.56 : 0.73 0.05 1.57\n",
      "mean norm:  0.13006238639354706 0.00662734592333436\n",
      "epoch: 41, loss: 1.8401141166687012, accruacy: 0.3194\n",
      "1.06 0.04 1.56 : 0.77 0.05 1.57\n",
      "mean norm:  0.13194392621517181 0.006524818949401379\n",
      "epoch: 42, loss: 1.94199538230896, accruacy: 0.3263\n",
      "1.06 0.04 1.53 : 0.75 0.05 1.57\n",
      "mean norm:  0.1324879378080368 0.0063995784148573875\n",
      "epoch: 43, loss: 1.8714433908462524, accruacy: 0.3328\n",
      "1.05 0.04 1.51 : 0.72 0.05 1.57\n",
      "mean norm:  0.1359715461730957 0.006597519852221012\n",
      "epoch: 44, loss: 1.9766526222229004, accruacy: 0.3212\n",
      "1.05 0.05 1.50 : 0.74 0.06 1.57\n",
      "mean norm:  0.1384381353855133 0.006494269706308842\n",
      "epoch: 45, loss: 1.9026867151260376, accruacy: 0.3381\n",
      "1.17 0.88 1.49 : 0.73 0.05 1.57\n",
      "mean norm:  0.14135628938674927 0.0067636617459356785\n",
      "epoch: 46, loss: 1.845968246459961, accruacy: 0.3097\n",
      "1.03 0.03 1.47 : 0.74 0.05 1.57\n",
      "mean norm:  0.14424119889736176 0.006634279619902372\n",
      "epoch: 47, loss: 1.850286841392517, accruacy: 0.2957\n",
      "1.04 0.04 1.48 : 0.74 0.05 1.57\n",
      "mean norm:  0.14615444839000702 0.006340149324387312\n",
      "epoch: 48, loss: 1.951521635055542, accruacy: 0.3345\n",
      "1.06 0.04 1.54 : 0.73 0.05 1.57\n",
      "mean norm:  0.15005333721637726 0.006576307117938995\n",
      "epoch: 49, loss: 1.8563429117202759, accruacy: 0.336\n"
     ]
    }
   ],
   "source": [
    "# ADK UPDATE double\n",
    "epochs = 50\n",
    "lamd = 0.01\n",
    "lr = 0.01\n",
    "target_norm = 0.1\n",
    "theta = 1.\n",
    "rtn = 1\n",
    "\n",
    "model = TestModel().cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "for e in range(epochs):\n",
    "    for x, y in train_loader:\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "        out = model(x)\n",
    "       \n",
    "        W = torch.t(model.layer3.weight)\n",
    "\n",
    "        WWT = W @ torch.t(W)\n",
    "        norm2 = torch.diagonal(WWT, 0)\n",
    "        nloss = torch.sum((target_norm**2 - norm2)**2)        \n",
    "        \n",
    "        loss = criterion(out, y) + lamd*nloss\n",
    "#         loss = nl\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "        weight1 = W.clone()\n",
    "        tloss = ad_function(weight1, theta, target_norm=0.1)\n",
    "        delW = torch.autograd.grad(tloss, weight1)[0]\n",
    "#         print('norm2', norm2)\n",
    "#         print('1', W)\n",
    "        with torch.no_grad():\n",
    "            newW = weight1 - lr*lamd*rtn*(delW*torch.norm(weight1, dim=[1], keepdim=True)**2).view(weight1.shape)\n",
    "            model.layer3.weight.copy_(torch.t(newW))\n",
    "        \n",
    "#         print('2', W)\n",
    "#         print('delW', delW)\n",
    "#         print('norm', torch.norm(weight1, dim=[1], keepdim=True))\n",
    "#         print('delW/norm', (delW*torch.norm(weight1, dim=[1], keepdim=True)**2).view(weight1.shape))\n",
    "        \n",
    "        \n",
    "    W = model.layer3.weight\n",
    "    Mean, Min, Max = angle_analysis(W)\n",
    "    TMean, TMin, TMax = angle_analysis(torch.t(W))\n",
    "    print(f'{Mean.item():3.2f} {Min.item():3.2f} {Max.item():3.2f} : {TMean.item():3.2f} {TMin.item():3.2f} {TMax.item():3.2f}')\n",
    "    print('mean norm: ', torch.mean(torch.norm(W, dim=1)).item(), torch.mean(torch.norm(W, dim=0)).item())\n",
    "    test_num = 0\n",
    "    hit_num = 0\n",
    "    for x, y in val_loader:\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "        test_num += len(y)\n",
    "        out = model(x)\n",
    "        pred = out.argmax(dim=1, keepdim=True)\n",
    "        hit_num += pred.eq(y.view_as(pred)).sum().item()\n",
    "    print(f'epoch: {e}, loss: {loss}, accruacy: {hit_num/test_num}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "dfbbfcde",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.22 0.04 1.54 : 0.89 0.26 1.57\n",
      "mean norm:  0.009849299676716328 0.0009317856165580451\n",
      "epoch: 0, loss: 2.2972638607025146, accruacy: 0.1811\n",
      "1.16 0.04 1.55 : 0.90 0.26 1.57\n",
      "mean norm:  0.007223817054182291 0.0006416146643459797\n",
      "epoch: 1, loss: 2.299117088317871, accruacy: 0.1865\n",
      "1.28 0.98 1.55 : 0.90 0.22 1.57\n",
      "mean norm:  0.007736501283943653 0.0006800131523050368\n",
      "epoch: 2, loss: 2.2951574325561523, accruacy: 0.1601\n",
      "1.11 0.05 1.53 : 0.89 0.20 1.57\n",
      "mean norm:  0.007617245893925428 0.0006699476507492363\n",
      "epoch: 3, loss: 2.296225070953369, accruacy: 0.217\n",
      "1.24 0.87 1.54 : 0.87 0.17 1.57\n",
      "mean norm:  0.008078524842858315 0.0007082631927914917\n",
      "epoch: 4, loss: 2.2950332164764404, accruacy: 0.2241\n",
      "1.16 0.90 1.39 : 0.88 0.19 1.57\n",
      "mean norm:  0.008235028944909573 0.0007216797093860805\n",
      "epoch: 5, loss: 2.2955429553985596, accruacy: 0.2412\n",
      "1.12 0.05 1.54 : 0.88 0.21 1.57\n",
      "mean norm:  0.008314087986946106 0.0007314324611797929\n",
      "epoch: 6, loss: 2.293825149536133, accruacy: 0.222\n",
      "1.09 0.05 1.49 : 0.87 0.12 1.57\n",
      "mean norm:  0.008970582857728004 0.0007885780651122332\n",
      "epoch: 7, loss: 2.2921760082244873, accruacy: 0.2071\n",
      "1.18 0.83 1.39 : 0.86 0.19 1.57\n",
      "mean norm:  0.008843990042805672 0.0007761644665151834\n",
      "epoch: 8, loss: 2.2937963008880615, accruacy: 0.2551\n",
      "1.26 0.88 1.53 : 0.86 0.19 1.57\n",
      "mean norm:  0.008933978155255318 0.0007842222112230957\n",
      "epoch: 9, loss: 2.2929763793945312, accruacy: 0.2049\n",
      "1.13 0.05 1.56 : 0.87 0.18 1.57\n",
      "mean norm:  0.00907677598297596 0.000798271328676492\n",
      "epoch: 10, loss: 2.291653871536255, accruacy: 0.2443\n",
      "1.18 0.83 1.55 : 0.87 0.18 1.57\n",
      "mean norm:  0.01009136438369751 0.0008764549857005477\n",
      "epoch: 11, loss: 2.286137819290161, accruacy: 0.2274\n",
      "1.19 0.80 1.51 : 0.85 0.19 1.57\n",
      "mean norm:  0.009947269223630428 0.0008668288355693221\n",
      "epoch: 12, loss: 2.2889318466186523, accruacy: 0.2307\n",
      "1.23 0.84 1.51 : 0.85 0.17 1.57\n",
      "mean norm:  0.010282169096171856 0.0008967046160250902\n",
      "epoch: 13, loss: 2.2861459255218506, accruacy: 0.2196\n",
      "1.20 0.80 1.48 : 0.83 0.10 1.57\n",
      "mean norm:  0.010828481987118721 0.0009494713740423322\n",
      "epoch: 14, loss: 2.280557870864868, accruacy: 0.1755\n",
      "1.19 0.83 1.50 : 0.85 0.10 1.57\n",
      "mean norm:  0.010777737945318222 0.0009421583963558078\n",
      "epoch: 15, loss: 2.2822022438049316, accruacy: 0.2283\n",
      "1.10 0.05 1.48 : 0.84 0.14 1.57\n",
      "mean norm:  0.011105398647487164 0.0009765897411853075\n",
      "epoch: 16, loss: 2.279794216156006, accruacy: 0.2153\n",
      "1.14 0.83 1.46 : 0.83 0.16 1.57\n",
      "mean norm:  0.011627203784883022 0.001017348957248032\n",
      "epoch: 17, loss: 2.274805784225464, accruacy: 0.2075\n",
      "1.15 0.78 1.54 : 0.82 0.11 1.57\n",
      "mean norm:  0.011874950490891933 0.001035720924846828\n",
      "epoch: 18, loss: 2.2764244079589844, accruacy: 0.215\n",
      "1.05 0.04 1.39 : 0.83 0.15 1.57\n",
      "mean norm:  0.012502484023571014 0.0010911651188507676\n",
      "epoch: 19, loss: 2.270781993865967, accruacy: 0.2156\n",
      "1.07 0.05 1.48 : 0.84 0.19 1.57\n",
      "mean norm:  0.012486825697124004 0.0010958827333524823\n",
      "epoch: 20, loss: 2.2696352005004883, accruacy: 0.2258\n",
      "1.23 0.76 1.55 : 0.83 0.10 1.57\n",
      "mean norm:  0.013608700595796108 0.0011882665567100048\n",
      "epoch: 21, loss: 2.265944480895996, accruacy: 0.2338\n",
      "1.21 0.72 1.56 : 0.84 0.09 1.57\n",
      "mean norm:  0.013777094893157482 0.0012008975027129054\n",
      "epoch: 22, loss: 2.255190134048462, accruacy: 0.2086\n",
      "0.97 0.05 1.34 : 0.84 0.16 1.57\n",
      "mean norm:  0.014079868793487549 0.0012362998677417636\n",
      "epoch: 23, loss: 2.248244047164917, accruacy: 0.2308\n",
      "1.13 0.82 1.49 : 0.82 0.15 1.57\n",
      "mean norm:  0.013975031673908234 0.0012333550257608294\n",
      "epoch: 24, loss: 2.2557671070098877, accruacy: 0.2348\n",
      "1.13 0.77 1.43 : 0.81 0.08 1.57\n",
      "mean norm:  0.015475264750421047 0.0013654121430590749\n",
      "epoch: 25, loss: 2.23531174659729, accruacy: 0.1896\n",
      "1.15 0.76 1.45 : 0.81 0.13 1.57\n",
      "mean norm:  0.015754012390971184 0.0013786668423563242\n",
      "epoch: 26, loss: 2.229339122772217, accruacy: 0.2204\n",
      "1.10 0.04 1.50 : 0.83 0.15 1.57\n",
      "mean norm:  0.015619099140167236 0.0013772195670753717\n",
      "epoch: 27, loss: 2.225220203399658, accruacy: 0.1965\n",
      "1.03 0.05 1.48 : 0.82 0.08 1.57\n",
      "mean norm:  0.016121756285429 0.001410845434293151\n",
      "epoch: 28, loss: 2.226646661758423, accruacy: 0.2095\n",
      "1.13 0.78 1.50 : 0.81 0.13 1.57\n",
      "mean norm:  0.0171955656260252 0.0015298525104299188\n",
      "epoch: 29, loss: 2.199531078338623, accruacy: 0.1838\n",
      "1.26 0.82 1.53 : 0.82 0.08 1.57\n",
      "mean norm:  0.016716714948415756 0.001471031573601067\n",
      "epoch: 30, loss: 2.2188689708709717, accruacy: 0.2315\n",
      "1.23 0.82 1.57 : 0.81 0.14 1.57\n",
      "mean norm:  0.017693093046545982 0.001545977545902133\n",
      "epoch: 31, loss: 2.199113368988037, accruacy: 0.196\n",
      "1.17 0.81 1.56 : 0.82 0.07 1.57\n",
      "mean norm:  0.018505532294511795 0.0016140973893925548\n",
      "epoch: 32, loss: 2.1961333751678467, accruacy: 0.2216\n",
      "1.11 0.79 1.40 : 0.81 0.15 1.57\n",
      "mean norm:  0.018908176571130753 0.0016489444533362985\n",
      "epoch: 33, loss: 2.2073261737823486, accruacy: 0.2306\n",
      "1.04 0.05 1.55 : 0.83 0.08 1.57\n",
      "mean norm:  0.0185930784791708 0.0016231411136686802\n",
      "epoch: 34, loss: 2.2185068130493164, accruacy: 0.2328\n",
      "1.15 0.85 1.47 : 0.81 0.13 1.57\n",
      "mean norm:  0.019903097301721573 0.0017468706937506795\n",
      "epoch: 35, loss: 2.183370351791382, accruacy: 0.1949\n",
      "1.23 0.84 1.56 : 0.84 0.15 1.57\n",
      "mean norm:  0.019311124458909035 0.0016905509401112795\n",
      "epoch: 36, loss: 2.2157187461853027, accruacy: 0.2117\n",
      "1.22 0.83 1.55 : 0.82 0.17 1.57\n",
      "mean norm:  0.01960003934800625 0.0017175195971503854\n",
      "epoch: 37, loss: 2.18277907371521, accruacy: 0.2227\n",
      "1.24 0.89 1.55 : 0.82 0.15 1.57\n",
      "mean norm:  0.019558783620595932 0.0017071590991690755\n",
      "epoch: 38, loss: 2.1773462295532227, accruacy: 0.2053\n",
      "1.21 0.84 1.51 : 0.81 0.07 1.57\n",
      "mean norm:  0.02085423283278942 0.0018286503618583083\n",
      "epoch: 39, loss: 2.158337116241455, accruacy: 0.213\n",
      "1.12 0.04 1.54 : 0.81 0.08 1.57\n",
      "mean norm:  0.020344143733382225 0.0017921451944857836\n",
      "epoch: 40, loss: 2.1692776679992676, accruacy: 0.1811\n",
      "1.02 0.04 1.43 : 0.79 0.06 1.57\n",
      "mean norm:  0.022856825962662697 0.0020083903800696135\n",
      "epoch: 41, loss: 2.1287946701049805, accruacy: 0.2173\n",
      "1.16 0.86 1.57 : 0.81 0.15 1.57\n",
      "mean norm:  0.021045519039034843 0.0018532242393121123\n",
      "epoch: 42, loss: 2.144258737564087, accruacy: 0.1923\n",
      "1.10 0.05 1.56 : 0.82 0.06 1.57\n",
      "mean norm:  0.021345850080251694 0.0018930122023448348\n",
      "epoch: 43, loss: 2.1637792587280273, accruacy: 0.2178\n",
      "1.23 0.87 1.56 : 0.81 0.16 1.57\n",
      "mean norm:  0.021853169426321983 0.0019038866739720106\n",
      "epoch: 44, loss: 2.155834913253784, accruacy: 0.2225\n",
      "1.19 0.87 1.52 : 0.81 0.15 1.57\n",
      "mean norm:  0.021755719557404518 0.0019120343495160341\n",
      "epoch: 45, loss: 2.1531364917755127, accruacy: 0.1944\n",
      "1.08 0.05 1.45 : 0.80 0.13 1.57\n",
      "mean norm:  0.022841235622763634 0.001997346756979823\n",
      "epoch: 46, loss: 2.1027848720550537, accruacy: 0.2376\n",
      "1.09 0.04 1.52 : 0.79 0.14 1.57\n",
      "mean norm:  0.02302098274230957 0.0019961954094469547\n",
      "epoch: 47, loss: 2.1592657566070557, accruacy: 0.236\n",
      "1.05 0.04 1.52 : 0.81 0.13 1.57\n",
      "mean norm:  0.022862039506435394 0.0019866968505084515\n",
      "epoch: 48, loss: 2.139113664627075, accruacy: 0.219\n",
      "1.10 0.82 1.52 : 0.80 0.14 1.57\n",
      "mean norm:  0.023926109075546265 0.0021127157378941774\n",
      "epoch: 49, loss: 2.0951781272888184, accruacy: 0.22\n"
     ]
    }
   ],
   "source": [
    "# ADK UPDATE double\n",
    "epochs = 50\n",
    "lamd = 0.01\n",
    "lr = 0.01\n",
    "target_norm = 0.1\n",
    "theta = 1.57\n",
    "rtn = 1\n",
    "\n",
    "model = TestModel().cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "for e in range(epochs):\n",
    "    for x, y in train_loader:\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "        out = model(x)\n",
    "       \n",
    "        W = torch.t(model.layer3.weight)\n",
    "\n",
    "        WWT = W @ torch.t(W)\n",
    "        norm2 = torch.diagonal(WWT, 0)\n",
    "        nloss = torch.sum((target_norm**2 - norm2)**2)        \n",
    "        \n",
    "        loss = criterion(out, y) + lamd*nloss\n",
    "#         loss = nl\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "        weight1 = W.clone()\n",
    "        tloss = ad_function(weight1, theta, target_norm=0.1)\n",
    "        delW = torch.autograd.grad(tloss, weight1)[0]\n",
    "#         print('norm2', norm2)\n",
    "#         print('1', W)\n",
    "        with torch.no_grad():\n",
    "            newW = weight1 - lr*lamd*rtn*(delW*torch.norm(weight1, dim=[1], keepdim=True)**2).view(weight1.shape)\n",
    "            model.layer3.weight.copy_(torch.t(newW))\n",
    "        \n",
    "#         print('2', W)\n",
    "#         print('delW', delW)\n",
    "#         print('norm', torch.norm(weight1, dim=[1], keepdim=True))\n",
    "#         print('delW/norm', (delW*torch.norm(weight1, dim=[1], keepdim=True)**2).view(weight1.shape))\n",
    "        \n",
    "        \n",
    "    W = model.layer3.weight\n",
    "    Mean, Min, Max = angle_analysis(W)\n",
    "    TMean, TMin, TMax = angle_analysis(torch.t(W))\n",
    "    print(f'{Mean.item():3.2f} {Min.item():3.2f} {Max.item():3.2f} : {TMean.item():3.2f} {TMin.item():3.2f} {TMax.item():3.2f}')\n",
    "    print('mean norm: ', torch.mean(torch.norm(W, dim=1)).item(), torch.mean(torch.norm(W, dim=0)).item())\n",
    "    test_num = 0\n",
    "    hit_num = 0\n",
    "    for x, y in val_loader:\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "        test_num += len(y)\n",
    "        out = model(x)\n",
    "        pred = out.argmax(dim=1, keepdim=True)\n",
    "        hit_num += pred.eq(y.view_as(pred)).sum().item()\n",
    "    print(f'epoch: {e}, loss: {loss}, accruacy: {hit_num/test_num}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42095545",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96295bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1f8689",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043638fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADK UPDATE double\n",
    "lamd = 1\n",
    "lr = 0.01\n",
    "target_norm = 0.1\n",
    "model = TestModel().cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "for e in range(epochs):\n",
    "    for x, y in train_loader:\n",
    "        print('######')\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "        out = model(x)\n",
    "       \n",
    "        W = model.layer3.weight\n",
    "        WWT = W @ torch.t(W)\n",
    "        norm2 = torch.diagonal(WWT, 0)\n",
    "        nloss = torch.sum((target_norm**2 - norm2)**2)        \n",
    "        \n",
    "        loss = criterion(out, y) + lamd*nl \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "        weight1 = W.clone()\n",
    "        tloss = ad_function(torch.t(weight1), theta, target_norm=0.1)\n",
    "        delW = torch.autograd.grad(tloss, weight1)[0]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            newW = weight1 - lr*lamd*(delW/torch.norm(weight1, dim=[1], keepdim=True)).view(weight1.shape)\n",
    "            W.copy_(newW)      \n",
    "        \n",
    "        \n",
    "    W = model.layer3.weight\n",
    "    Mean, Min, Max = angle_analysis(W)\n",
    "    TMean, TMin, TMax = angle_analysis(torch.t(W))\n",
    "    print(f'{Mean.item():3.2f} {Min.item():3.2f} {Max.item():3.2f} : {TMean.item():3.2f} {TMin.item():3.2f} {TMax.item():3.2f}')\n",
    "    print('mean norm: ', torch.mean(torch.norm(W, dim=1)).item(), torch.mean(torch.norm(W, dim=0)).item())\n",
    "    test_num = 0\n",
    "    hit_num = 0\n",
    "    for x, y in val_loader:\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "        test_num += len(y)\n",
    "        out = model(x)\n",
    "        pred = out.argmax(dim=1, keepdim=True)\n",
    "        hit_num += pred.eq(y.view_as(pred)).sum().item()\n",
    "    print(f'epoch: {e}, loss: {loss}, accruacy: {hit_num/test_num}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecc156b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bcada5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ba10d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422ed244",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "fcd2c84a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.52 1.49 1.57 : 0.88 0.04 1.57\n",
      "mean norm:  0.587649941444397 0.05807867646217346\n",
      "epoch: 0, loss: 2.1342148780822754, accruacy: 0.2645\n",
      "1.37 0.04 1.57 : 0.88 0.04 1.57\n",
      "mean norm:  0.6067554354667664 0.05987248569726944\n",
      "epoch: 1, loss: 1.9973318576812744, accruacy: 0.2867\n",
      "1.35 0.04 1.57 : 0.89 0.04 1.57\n",
      "mean norm:  0.6234175562858582 0.06141820177435875\n",
      "epoch: 2, loss: 2.0336532592773438, accruacy: 0.3089\n",
      "1.33 0.05 1.55 : 0.89 0.04 1.57\n",
      "mean norm:  0.6380568146705627 0.06277431547641754\n",
      "epoch: 3, loss: 1.9372048377990723, accruacy: 0.3159\n",
      "1.32 0.05 1.56 : 0.89 0.04 1.57\n",
      "mean norm:  0.6519581079483032 0.06406112760305405\n",
      "epoch: 4, loss: 1.8804259300231934, accruacy: 0.3271\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-212-54dcf35c1815>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1186\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1187\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1140\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1142\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1143\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    171\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ADK double\n",
    "model = TestModel().cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "for e in range(epochs):\n",
    "    for x, y in train_loader:\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "        out = model(x)\n",
    "        nl, tl = ADK(model.layer3.weight, 1.0, double=True)\n",
    "        loss = criterion(out, y) + 0.1*nl + 0.*tl\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    W = model.layer3.weight\n",
    "    Mean, Min, Max = angle_analysis(W)\n",
    "    TMean, TMin, TMax = angle_analysis(torch.t(W))\n",
    "    print(f'{Mean.item():3.2f} {Min.item():3.2f} {Max.item():3.2f} : {TMean.item():3.2f} {TMin.item():3.2f} {TMax.item():3.2f}')\n",
    "    print('mean norm: ', torch.mean(torch.norm(W, dim=1)).item(), torch.mean(torch.norm(W, dim=0)).item())\n",
    "    test_num = 0\n",
    "    hit_num = 0\n",
    "    for x, y in val_loader:\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "        test_num += len(y)\n",
    "        out = model(x)\n",
    "        pred = out.argmax(dim=1, keepdim=True)\n",
    "        hit_num += pred.eq(y.view_as(pred)).sum().item()\n",
    "    print(f'epoch: {e}, loss: {loss}, accruacy: {hit_num/test_num}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "8cf60e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan nan nan : nan nan nan\n",
      "mean norm:  nan nan\n",
      "epoch: 0, loss: nan, accruacy: 0.1\n",
      "nan nan nan : nan nan nan\n",
      "mean norm:  nan nan\n",
      "epoch: 1, loss: nan, accruacy: 0.1\n",
      "nan nan nan : nan nan nan\n",
      "mean norm:  nan nan\n",
      "epoch: 2, loss: nan, accruacy: 0.1\n",
      "nan nan nan : nan nan nan\n",
      "mean norm:  nan nan\n",
      "epoch: 3, loss: nan, accruacy: 0.1\n",
      "nan nan nan : nan nan nan\n",
      "mean norm:  nan nan\n",
      "epoch: 4, loss: nan, accruacy: 0.1\n"
     ]
    }
   ],
   "source": [
    "# ADK double\n",
    "model = TestModel().cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "for e in range(epochs):\n",
    "    for x, y in train_loader:\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "        out = model(x)\n",
    "        nl, tl = ADK(model.layer3.weight, 1.0, double=True)\n",
    "        loss = criterion(out, y) + 1*nl + 1*tl\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    W = model.layer3.weight\n",
    "    Mean, Min, Max = angle_analysis(W)\n",
    "    TMean, TMin, TMax = angle_analysis(torch.t(W))\n",
    "    print(f'{Mean.item():3.2f} {Min.item():3.2f} {Max.item():3.2f} : {TMean.item():3.2f} {TMin.item():3.2f} {TMax.item():3.2f}')\n",
    "    print('mean norm: ', torch.mean(torch.norm(W, dim=1)).item(), torch.mean(torch.norm(W, dim=0)).item())\n",
    "    test_num = 0\n",
    "    hit_num = 0\n",
    "    for x, y in val_loader:\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "        test_num += len(y)\n",
    "        out = model(x)\n",
    "        pred = out.argmax(dim=1, keepdim=True)\n",
    "        hit_num += pred.eq(y.view_as(pred)).sum().item()\n",
    "    print(f'epoch: {e}, loss: {loss}, accruacy: {hit_num/test_num}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd9fd57d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######################\n",
      "###########   0.1   ###########\n",
      "1.50 0.50\n",
      "epoch: 0, loss: 2.1134190559387207, accruacy: 0.2617\n",
      "1.46 0.50\n",
      "epoch: 1, loss: 2.032808542251587, accruacy: 0.2879\n",
      "1.43 0.50\n",
      "epoch: 2, loss: 2.004826784133911, accruacy: 0.3085\n",
      "1.41 0.50\n",
      "epoch: 3, loss: 1.9810376167297363, accruacy: 0.3206\n",
      "1.39 0.50\n",
      "epoch: 4, loss: 1.9487046003341675, accruacy: 0.3276\n",
      "1.38 0.50\n",
      "epoch: 5, loss: 1.7997831106185913, accruacy: 0.3351\n",
      "1.36 0.50\n",
      "epoch: 6, loss: 1.8502875566482544, accruacy: 0.3411\n",
      "1.35 0.50\n",
      "epoch: 7, loss: 1.8180392980575562, accruacy: 0.3463\n",
      "1.34 0.50\n",
      "epoch: 8, loss: 1.8315154314041138, accruacy: 0.3521\n",
      "1.34 0.50\n",
      "epoch: 9, loss: 1.7746917009353638, accruacy: 0.3551\n",
      "#######################\n",
      "###########   0.2   ###########\n",
      "1.51 0.50\n",
      "epoch: 0, loss: 2.138826370239258, accruacy: 0.2582\n",
      "1.47 0.50\n",
      "epoch: 1, loss: 2.7714345455169678, accruacy: 0.2875\n",
      "1.45 0.50\n",
      "epoch: 2, loss: 2.692617654800415, accruacy: 0.3083\n",
      "1.41 0.50\n",
      "epoch: 3, loss: 2.5747790336608887, accruacy: 0.3204\n",
      "1.34 0.50\n",
      "epoch: 4, loss: 2.545764446258545, accruacy: 0.3319\n",
      "1.25 0.50\n",
      "epoch: 5, loss: 2.3841352462768555, accruacy: 0.3403\n",
      "1.17 0.50\n",
      "epoch: 6, loss: 2.1807005405426025, accruacy: 0.3514\n",
      "1.12 0.50\n",
      "epoch: 7, loss: 2.1331679821014404, accruacy: 0.3561\n",
      "1.10 0.50\n",
      "epoch: 8, loss: 2.177994966506958, accruacy: 0.3632\n",
      "1.10 0.50\n",
      "epoch: 9, loss: 2.168118715286255, accruacy: 0.3671\n",
      "#######################\n",
      "###########   0.30000000000000004   ###########\n",
      "1.49 0.50\n",
      "epoch: 0, loss: 4.671985626220703, accruacy: 0.259\n",
      "1.45 0.50\n",
      "epoch: 1, loss: 5.121510982513428, accruacy: 0.2942\n",
      "1.38 0.50\n",
      "epoch: 2, loss: 5.579245567321777, accruacy: 0.3147\n",
      "1.28 0.50\n",
      "epoch: 3, loss: 5.454895496368408, accruacy: 0.3273\n",
      "1.19 0.50\n",
      "epoch: 4, loss: 5.329117298126221, accruacy: 0.3359\n",
      "1.14 0.50\n",
      "epoch: 5, loss: 5.137429237365723, accruacy: 0.3512\n",
      "1.14 0.50\n",
      "epoch: 6, loss: 4.650271892547607, accruacy: 0.3662\n",
      "1.16 0.50\n",
      "epoch: 7, loss: 4.38831901550293, accruacy: 0.3654\n",
      "1.18 0.50\n",
      "epoch: 8, loss: 4.492461204528809, accruacy: 0.3771\n",
      "1.19 0.50\n",
      "epoch: 9, loss: 5.025605201721191, accruacy: 0.381\n",
      "#######################\n",
      "###########   0.4   ###########\n",
      "1.45 0.48\n",
      "epoch: 0, loss: 20.409509658813477, accruacy: 0.2586\n",
      "1.24 0.47\n",
      "epoch: 1, loss: 23.50798225402832, accruacy: 0.2888\n",
      "1.11 0.46\n",
      "epoch: 2, loss: 21.387563705444336, accruacy: 0.3136\n",
      "1.14 0.46\n",
      "epoch: 3, loss: 19.09823989868164, accruacy: 0.3265\n",
      "1.21 0.46\n",
      "epoch: 4, loss: 16.50885009765625, accruacy: 0.3555\n",
      "1.26 0.46\n",
      "epoch: 5, loss: 14.449348449707031, accruacy: 0.3486\n",
      "1.28 0.46\n",
      "epoch: 6, loss: 12.792689323425293, accruacy: 0.3626\n",
      "1.30 0.46\n",
      "epoch: 7, loss: 12.199905395507812, accruacy: 0.3857\n",
      "1.30 0.46\n",
      "epoch: 8, loss: 12.153508186340332, accruacy: 0.3905\n",
      "1.31 0.46\n",
      "epoch: 9, loss: 11.934200286865234, accruacy: 0.3807\n",
      "#######################\n",
      "###########   0.5   ###########\n",
      "1.17 0.38\n",
      "epoch: 0, loss: 135.771240234375, accruacy: 0.2619\n",
      "1.24 0.32\n",
      "epoch: 1, loss: 86.80097198486328, accruacy: 0.3189\n",
      "1.32 0.30\n",
      "epoch: 2, loss: 57.02663040161133, accruacy: 0.3359\n",
      "1.35 0.30\n",
      "epoch: 3, loss: 47.98133850097656, accruacy: 0.385\n",
      "1.37 0.30\n",
      "epoch: 4, loss: 43.85662078857422, accruacy: 0.3945\n",
      "1.39 0.29\n",
      "epoch: 5, loss: 41.357696533203125, accruacy: 0.3965\n",
      "1.40 0.29\n",
      "epoch: 6, loss: 39.396575927734375, accruacy: 0.3996\n",
      "1.41 0.29\n",
      "epoch: 7, loss: 38.121517181396484, accruacy: 0.4087\n",
      "1.42 0.29\n",
      "epoch: 8, loss: 36.92914962768555, accruacy: 0.3935\n",
      "1.42 0.29\n",
      "epoch: 9, loss: 36.59443664550781, accruacy: 0.4157\n",
      "#######################\n",
      "###########   0.6000000000000001   ###########\n",
      "1.23 0.19\n",
      "epoch: 0, loss: 403.6351623535156, accruacy: 0.2921\n",
      "1.38 0.13\n",
      "epoch: 1, loss: 78.81927490234375, accruacy: 0.3624\n",
      "1.39 0.12\n",
      "epoch: 2, loss: 66.00130462646484, accruacy: 0.3505\n",
      "1.40 0.11\n",
      "epoch: 3, loss: 63.626861572265625, accruacy: 0.3769\n",
      "1.40 0.11\n",
      "epoch: 4, loss: 62.77706527709961, accruacy: 0.401\n",
      "1.40 0.11\n",
      "epoch: 5, loss: 62.280914306640625, accruacy: 0.3796\n",
      "1.40 0.11\n",
      "epoch: 6, loss: 62.20880889892578, accruacy: 0.408\n",
      "1.40 0.11\n",
      "epoch: 7, loss: 62.02170181274414, accruacy: 0.4141\n",
      "1.40 0.11\n",
      "epoch: 8, loss: 62.21592712402344, accruacy: 0.4167\n",
      "1.39 0.11\n",
      "epoch: 9, loss: 61.88355255126953, accruacy: 0.4103\n",
      "#######################\n",
      "###########   0.7000000000000001   ###########\n",
      "nan nan\n",
      "epoch: 0, loss: nan, accruacy: 0.1\n",
      "nan nan\n",
      "epoch: 1, loss: nan, accruacy: 0.1\n",
      "nan nan\n",
      "epoch: 2, loss: nan, accruacy: 0.1\n",
      "nan nan\n",
      "epoch: 3, loss: nan, accruacy: 0.1\n",
      "nan nan\n",
      "epoch: 4, loss: nan, accruacy: 0.1\n",
      "nan nan\n",
      "epoch: 5, loss: nan, accruacy: 0.1\n",
      "nan nan\n",
      "epoch: 6, loss: nan, accruacy: 0.1\n",
      "nan nan\n",
      "epoch: 7, loss: nan, accruacy: 0.1\n",
      "nan nan\n",
      "epoch: 8, loss: nan, accruacy: 0.1\n",
      "nan nan\n",
      "epoch: 9, loss: nan, accruacy: 0.1\n",
      "#######################\n",
      "###########   0.8   ###########\n",
      "nan nan\n",
      "epoch: 0, loss: nan, accruacy: 0.1\n",
      "nan nan\n",
      "epoch: 1, loss: nan, accruacy: 0.1\n",
      "nan nan\n",
      "epoch: 2, loss: nan, accruacy: 0.1\n",
      "nan nan\n",
      "epoch: 3, loss: nan, accruacy: 0.1\n",
      "nan nan\n",
      "epoch: 4, loss: nan, accruacy: 0.1\n",
      "nan nan\n",
      "epoch: 5, loss: nan, accruacy: 0.1\n",
      "nan nan\n",
      "epoch: 6, loss: nan, accruacy: 0.1\n",
      "nan nan\n",
      "epoch: 7, loss: nan, accruacy: 0.1\n",
      "nan nan\n",
      "epoch: 8, loss: nan, accruacy: 0.1\n",
      "nan nan\n",
      "epoch: 9, loss: nan, accruacy: 0.1\n",
      "#######################\n",
      "###########   0.9   ###########\n",
      "nan nan\n",
      "epoch: 0, loss: nan, accruacy: 0.1\n",
      "nan nan\n",
      "epoch: 1, loss: nan, accruacy: 0.1\n",
      "nan nan\n",
      "epoch: 2, loss: nan, accruacy: 0.1\n",
      "nan nan\n",
      "epoch: 3, loss: nan, accruacy: 0.1\n",
      "nan nan\n",
      "epoch: 4, loss: nan, accruacy: 0.1\n",
      "nan nan\n",
      "epoch: 5, loss: nan, accruacy: 0.1\n",
      "nan nan\n",
      "epoch: 6, loss: nan, accruacy: 0.1\n",
      "nan nan\n",
      "epoch: 7, loss: nan, accruacy: 0.1\n",
      "nan nan\n",
      "epoch: 8, loss: nan, accruacy: 0.1\n",
      "nan nan\n",
      "epoch: 9, loss: nan, accruacy: 0.1\n",
      "#######################\n",
      "###########   1.0   ###########\n",
      "nan nan\n",
      "epoch: 0, loss: nan, accruacy: 0.1\n",
      "nan nan\n",
      "epoch: 1, loss: nan, accruacy: 0.1\n",
      "nan nan\n",
      "epoch: 2, loss: nan, accruacy: 0.1\n",
      "nan nan\n",
      "epoch: 3, loss: nan, accruacy: 0.1\n",
      "nan nan\n",
      "epoch: 4, loss: nan, accruacy: 0.1\n",
      "nan nan\n",
      "epoch: 5, loss: nan, accruacy: 0.1\n",
      "nan nan\n",
      "epoch: 6, loss: nan, accruacy: 0.1\n",
      "nan nan\n",
      "epoch: 7, loss: nan, accruacy: 0.1\n",
      "nan nan\n",
      "epoch: 8, loss: nan, accruacy: 0.1\n",
      "nan nan\n",
      "epoch: 9, loss: nan, accruacy: 0.1\n",
      "#######################\n",
      "###########   1.1   ###########\n",
      "nan nan\n",
      "epoch: 0, loss: nan, accruacy: 0.1\n",
      "nan nan\n",
      "epoch: 1, loss: nan, accruacy: 0.1\n",
      "nan nan\n",
      "epoch: 2, loss: nan, accruacy: 0.1\n",
      "nan nan\n",
      "epoch: 3, loss: nan, accruacy: 0.1\n",
      "nan nan\n",
      "epoch: 4, loss: nan, accruacy: 0.1\n",
      "nan nan\n",
      "epoch: 5, loss: nan, accruacy: 0.1\n",
      "nan nan\n",
      "epoch: 6, loss: nan, accruacy: 0.1\n",
      "nan nan\n",
      "epoch: 7, loss: nan, accruacy: 0.1\n",
      "nan nan\n",
      "epoch: 8, loss: nan, accruacy: 0.1\n",
      "nan nan\n",
      "epoch: 9, loss: nan, accruacy: 0.1\n",
      "#######################\n",
      "###########   1.2000000000000002   ###########\n",
      "1.29 0.44\n",
      "epoch: 0, loss: 5431.69482421875, accruacy: 0.2656\n",
      "0.78 0.18\n",
      "epoch: 1, loss: 233.1138458251953, accruacy: 0.3023\n",
      "0.42 0.09\n",
      "epoch: 2, loss: 1273.5682373046875, accruacy: 0.315\n",
      "0.35 0.07\n",
      "epoch: 3, loss: 3183.18896484375, accruacy: 0.3057\n",
      "0.31 0.06\n",
      "epoch: 4, loss: 1247.3287353515625, accruacy: 0.3071\n",
      "0.30 0.06\n",
      "epoch: 5, loss: 3183.176513671875, accruacy: 0.3406\n",
      "0.29 0.05\n",
      "epoch: 6, loss: 1246.98876953125, accruacy: 0.3291\n",
      "0.29 0.05\n",
      "epoch: 7, loss: 3182.68603515625, accruacy: 0.3422\n",
      "0.27 0.05\n",
      "epoch: 8, loss: 1247.0384521484375, accruacy: 0.358\n",
      "0.28 0.05\n",
      "epoch: 9, loss: 3183.25634765625, accruacy: 0.367\n",
      "#######################\n",
      "###########   1.3   ###########\n",
      "0.41 0.09\n",
      "epoch: 0, loss: 1136.8109130859375, accruacy: 0.2845\n",
      "0.31 0.07\n",
      "epoch: 1, loss: 1135.71484375, accruacy: 0.2986\n",
      "0.26 0.06\n",
      "epoch: 2, loss: 1135.5284423828125, accruacy: 0.3039\n",
      "0.22 0.06\n",
      "epoch: 3, loss: 1135.437255859375, accruacy: 0.3168\n",
      "0.20 0.06\n",
      "epoch: 4, loss: 1135.361328125, accruacy: 0.3224\n",
      "0.18 0.05\n",
      "epoch: 5, loss: 1135.32861328125, accruacy: 0.319\n",
      "0.17 0.05\n",
      "epoch: 6, loss: 1135.41943359375, accruacy: 0.3279\n",
      "0.16 0.05\n",
      "epoch: 7, loss: 1135.265380859375, accruacy: 0.3372\n",
      "0.15 0.05\n",
      "epoch: 8, loss: 1135.2685546875, accruacy: 0.3473\n",
      "0.15 0.05\n",
      "epoch: 9, loss: 1135.2833251953125, accruacy: 0.349\n",
      "#######################\n",
      "###########   1.4000000000000001   ###########\n",
      "0.63 0.13\n",
      "epoch: 0, loss: 6.389708995819092, accruacy: 0.272\n",
      "0.50 0.10\n",
      "epoch: 1, loss: 5.374631881713867, accruacy: 0.2596\n",
      "0.43 0.08\n",
      "epoch: 2, loss: 5.0928144454956055, accruacy: 0.3272\n",
      "0.38 0.08\n",
      "epoch: 3, loss: 4.963663578033447, accruacy: 0.332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35 0.07\n",
      "epoch: 4, loss: 4.947579860687256, accruacy: 0.3229\n",
      "0.33 0.07\n",
      "epoch: 5, loss: 5.0513834953308105, accruacy: 0.2616\n",
      "0.31 0.06\n",
      "epoch: 6, loss: 4.8058390617370605, accruacy: 0.3462\n",
      "0.30 0.06\n",
      "epoch: 7, loss: 4.797784805297852, accruacy: 0.3461\n",
      "0.28 0.06\n",
      "epoch: 8, loss: 4.810888767242432, accruacy: 0.3527\n",
      "0.27 0.06\n",
      "epoch: 9, loss: 4.7766852378845215, accruacy: 0.3571\n",
      "#######################\n",
      "###########   1.5   ###########\n",
      "0.78 0.20\n",
      "epoch: 0, loss: 4.666886806488037, accruacy: 0.2857\n",
      "0.67 0.14\n",
      "epoch: 1, loss: 2.9216794967651367, accruacy: 0.3195\n",
      "0.61 0.12\n",
      "epoch: 2, loss: 2.6076865196228027, accruacy: 0.3225\n",
      "0.57 0.11\n",
      "epoch: 3, loss: 2.551128387451172, accruacy: 0.3415\n",
      "0.54 0.10\n",
      "epoch: 4, loss: 2.5298352241516113, accruacy: 0.33\n",
      "0.52 0.10\n",
      "epoch: 5, loss: 2.441100835800171, accruacy: 0.3544\n",
      "0.50 0.09\n",
      "epoch: 6, loss: 2.3321566581726074, accruacy: 0.3618\n",
      "0.49 0.09\n",
      "epoch: 7, loss: 2.44405460357666, accruacy: 0.3612\n",
      "0.47 0.09\n",
      "epoch: 8, loss: 2.3308355808258057, accruacy: 0.3746\n",
      "0.46 0.08\n",
      "epoch: 9, loss: 2.485865592956543, accruacy: 0.3752\n",
      "#######################\n",
      "###########   1.6   ###########\n",
      "1.37 0.52\n",
      "epoch: 0, loss: 93.88249969482422, accruacy: 0.2171\n",
      "0.80 0.49\n",
      "epoch: 1, loss: 94.7734146118164, accruacy: 0.2274\n",
      "0.56 0.48\n",
      "epoch: 2, loss: 94.0284194946289, accruacy: 0.2315\n",
      "0.51 0.50\n",
      "epoch: 3, loss: 94.11392211914062, accruacy: 0.2479\n",
      "0.52 0.52\n",
      "epoch: 4, loss: 93.11335754394531, accruacy: 0.2565\n",
      "0.56 0.54\n",
      "epoch: 5, loss: 93.49024200439453, accruacy: 0.2633\n",
      "0.60 0.58\n",
      "epoch: 6, loss: 93.24777221679688, accruacy: 0.2651\n",
      "0.64 0.58\n",
      "epoch: 7, loss: 93.17611694335938, accruacy: 0.2709\n",
      "0.68 0.62\n",
      "epoch: 8, loss: 93.1471939086914, accruacy: 0.2777\n",
      "0.71 0.61\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-f5951559c110>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mtest_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mhit_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1186\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1187\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1140\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1142\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1143\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    171\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(1, 20):\n",
    "    u = i*0.1\n",
    "    print('###########  ', u, '  ###########')\n",
    "    # ADK double\n",
    "    model = TestModel().cuda()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "    for e in range(epochs):\n",
    "        for x, y in train_loader:\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "            out = model(x)\n",
    "            nl, tl = ADK(model.layer3.weight, u, double=True)\n",
    "            loss = criterion(out, y) + 0.1*nl + 0.1*tl\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        W = model.layer3.weight\n",
    "        Mean, Min, Max = angle_analysis(W)\n",
    "        TMean, TMin, TMax = angle_analysis(torch.t(W))\n",
    "        print(f'{Mean.item():3.2f} {Min.item():3.2f} {Max.item():3.2f} : {TMean.item():3.2f} {TMin.item():3.2f} {TMax.item():3.2f}')\n",
    "        test_num = 0\n",
    "        hit_num = 0\n",
    "        for x, y in val_loader:\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "            test_num += len(y)\n",
    "            out = model(x)\n",
    "            pred = out.argmax(dim=1, keepdim=True)\n",
    "            hit_num += pred.eq(y.view_as(pred)).sum().item()\n",
    "        print(f'epoch: {e}, loss: {loss}, accruacy: {hit_num/test_num}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7fc179a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.53 1.49 1.56 : 0.90 0.04 1.57\n",
      "mean norm:  0.589784562587738 0.05836125463247299\n",
      "epoch: 0, loss: 2.1580748558044434, accruacy: 0.2593\n",
      "1.37 0.05 1.57 : 0.90 0.04 1.57\n",
      "mean norm:  0.6068843007087708 0.05998790264129639\n",
      "epoch: 1, loss: 2.070744514465332, accruacy: 0.2941\n",
      "1.36 0.04 1.57 : 0.90 0.04 1.57\n",
      "mean norm:  0.6219269037246704 0.06139586865901947\n",
      "epoch: 2, loss: 1.9798004627227783, accruacy: 0.3113\n",
      "1.35 0.04 1.56 : 0.90 0.04 1.57\n",
      "mean norm:  0.6346355676651001 0.06257417052984238\n",
      "epoch: 3, loss: 1.9303721189498901, accruacy: 0.3204\n",
      "1.34 0.05 1.55 : 0.89 0.04 1.57\n",
      "mean norm:  0.6468605399131775 0.06370298564434052\n",
      "epoch: 4, loss: 1.8194307088851929, accruacy: 0.3311\n"
     ]
    }
   ],
   "source": [
    "# base, weight_decay\n",
    "model = TestModel().cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=0.0001)\n",
    "for e in range(epochs):\n",
    "    for x, y in train_loader:\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    W = model.layer3.weight\n",
    "    Mean, Min, Max = angle_analysis(W)\n",
    "    TMean, TMin, TMax = angle_analysis(torch.t(W))\n",
    "    print(f'{Mean.item():3.2f} {Min.item():3.2f} {Max.item():3.2f} : {TMean.item():3.2f} {TMin.item():3.2f} {TMax.item():3.2f}')\n",
    "    print('mean norm: ', torch.mean(torch.norm(W, dim=1)).item(), torch.mean(torch.norm(W, dim=0)).item())\n",
    "    test_num = 0\n",
    "    hit_num = 0\n",
    "    for x, y in val_loader:\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "        test_num += len(y)\n",
    "        out = model(x)\n",
    "        pred = out.argmax(dim=1, keepdim=True)\n",
    "        hit_num += pred.eq(y.view_as(pred)).sum().item()\n",
    "    print(f'epoch: {e}, loss: {loss}, accruacy: {hit_num/test_num}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d22e9570",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.53 1.48 1.57 : 0.90 0.04 1.57\n",
      "mean norm:  0.5919156074523926 0.058548636734485626\n",
      "epoch: 0, loss: 2.119493246078491, accruacy: 0.2649\n",
      "1.36 0.05 1.55 : 0.91 0.04 1.57\n",
      "mean norm:  0.6089869737625122 0.0601668506860733\n",
      "epoch: 1, loss: 2.0129971504211426, accruacy: 0.2912\n",
      "1.33 0.05 1.53 : 0.92 0.04 1.57\n",
      "mean norm:  0.6243717670440674 0.06160520017147064\n",
      "epoch: 2, loss: 1.9601598978042603, accruacy: 0.3056\n",
      "1.33 0.04 1.57 : 0.91 0.04 1.57\n",
      "mean norm:  0.6373639106750488 0.06281175464391708\n",
      "epoch: 3, loss: 1.9458551406860352, accruacy: 0.3209\n",
      "1.32 0.04 1.54 : 0.91 0.04 1.57\n",
      "mean norm:  0.6497540473937988 0.06395592540502548\n",
      "epoch: 4, loss: 1.9300075769424438, accruacy: 0.331\n"
     ]
    }
   ],
   "source": [
    "# base\n",
    "model = TestModel().cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "for e in range(epochs):\n",
    "    for x, y in train_loader:\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    W = model.layer3.weight\n",
    "    Mean, Min, Max = angle_analysis(W)\n",
    "    TMean, TMin, TMax = angle_analysis(torch.t(W))\n",
    "    print(f'{Mean.item():3.2f} {Min.item():3.2f} {Max.item():3.2f} : {TMean.item():3.2f} {TMin.item():3.2f} {TMax.item():3.2f}')\n",
    "    print('mean norm: ', torch.mean(torch.norm(W, dim=1)).item(), torch.mean(torch.norm(W, dim=0)).item())\n",
    "    test_num = 0\n",
    "    hit_num = 0\n",
    "    for x, y in val_loader:\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "        test_num += len(y)\n",
    "        out = model(x)\n",
    "        pred = out.argmax(dim=1, keepdim=True)\n",
    "        hit_num += pred.eq(y.view_as(pred)).sum().item()\n",
    "    print(f'epoch: {e}, loss: {loss}, accruacy: {hit_num/test_num}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2ff74995",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.52 1.46 1.56 : 0.90 0.04 1.57\n",
      "mean norm:  0.7327307462692261 0.07248667627573013\n",
      "epoch: 0, loss: 2.2849724292755127, accruacy: 0.2619\n",
      "1.52 1.44 1.57 : 0.89 0.04 1.57\n",
      "mean norm:  0.8564189076423645 0.08467362821102142\n",
      "epoch: 1, loss: 2.0351409912109375, accruacy: 0.303\n",
      "1.52 1.43 1.57 : 0.90 0.04 1.57\n",
      "mean norm:  0.9326834678649902 0.09217473864555359\n",
      "epoch: 2, loss: 1.9080106019973755, accruacy: 0.322\n",
      "1.52 1.44 1.57 : 0.90 0.04 1.57\n",
      "mean norm:  0.972987949848175 0.09613069891929626\n",
      "epoch: 3, loss: 1.9408897161483765, accruacy: 0.3352\n",
      "1.52 1.45 1.57 : 0.90 0.04 1.57\n",
      "mean norm:  0.992944061756134 0.0980745330452919\n",
      "epoch: 4, loss: 1.8335623741149902, accruacy: 0.3461\n"
     ]
    }
   ],
   "source": [
    "# ADK not double\n",
    "model = TestModel().cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "for e in range(epochs):\n",
    "    for x, y in train_loader:\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "        out = model(x)\n",
    "        nl, tl = ADK(model.layer3.weight, l, double=False)\n",
    "        loss = criterion(out, y) + 0.1*nl + 0.01*tl\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    W = model.layer3.weight\n",
    "    Mean, Min, Max = angle_analysis(W)\n",
    "    TMean, TMin, TMax = angle_analysis(torch.t(W))\n",
    "    print(f'{Mean.item():3.2f} {Min.item():3.2f} {Max.item():3.2f} : {TMean.item():3.2f} {TMin.item():3.2f} {TMax.item():3.2f}')\n",
    "    print('mean norm: ', torch.mean(torch.norm(W, dim=1)).item(), torch.mean(torch.norm(W, dim=0)).item())\n",
    "    test_num = 0\n",
    "    hit_num = 0\n",
    "    for x, y in val_loader:\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "        test_num += len(y)\n",
    "        out = model(x)\n",
    "        pred = out.argmax(dim=1, keepdim=True)\n",
    "        hit_num += pred.eq(y.view_as(pred)).sum().item()\n",
    "    print(f'epoch: {e}, loss: {loss}, accruacy: {hit_num/test_num}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b890da0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.39 0.05 1.55 : 0.93 0.04 1.57\n",
      "mean norm:  0.7329638600349426 0.0724828839302063\n",
      "epoch: 0, loss: 2.3123581409454346, accruacy: 0.2677\n",
      "1.54 1.52 1.56 : 0.92 0.04 1.57\n",
      "mean norm:  0.8568193316459656 0.08468151092529297\n",
      "epoch: 1, loss: 2.0182077884674072, accruacy: 0.3021\n",
      "1.40 0.05 1.57 : 0.92 0.04 1.57\n",
      "mean norm:  0.9329319000244141 0.09217444062232971\n",
      "epoch: 2, loss: 1.901869773864746, accruacy: 0.3184\n",
      "1.25 0.04 1.57 : 0.92 0.04 1.57\n",
      "mean norm:  0.9734387397766113 0.09615301340818405\n",
      "epoch: 3, loss: 1.9324917793273926, accruacy: 0.3376\n",
      "1.26 0.04 1.57 : 0.92 0.04 1.57\n",
      "mean norm:  0.9937114119529724 0.09812876582145691\n",
      "epoch: 4, loss: 1.8722680807113647, accruacy: 0.3479\n"
     ]
    }
   ],
   "source": [
    "# ADK not double\n",
    "model = TestModel().cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "for e in range(epochs):\n",
    "    for x, y in train_loader:\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "        out = model(x)\n",
    "        nl, tl = ADK(model.layer3.weight, l, double=False)\n",
    "        loss = criterion(out, y) + 0.1*nl + 0.1*tl\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    W = model.layer3.weight\n",
    "    Mean, Min, Max = angle_analysis(W)\n",
    "    TMean, TMin, TMax = angle_analysis(torch.t(W))\n",
    "    print(f'{Mean.item():3.2f} {Min.item():3.2f} {Max.item():3.2f} : {TMean.item():3.2f} {TMin.item():3.2f} {TMax.item():3.2f}')\n",
    "    print('mean norm: ', torch.mean(torch.norm(W, dim=1)).item(), torch.mean(torch.norm(W, dim=0)).item())\n",
    "    test_num = 0\n",
    "    hit_num = 0\n",
    "    for x, y in val_loader:\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "        test_num += len(y)\n",
    "        out = model(x)\n",
    "        pred = out.argmax(dim=1, keepdim=True)\n",
    "        hit_num += pred.eq(y.view_as(pred)).sum().item()\n",
    "    print(f'epoch: {e}, loss: {loss}, accruacy: {hit_num/test_num}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "577ad654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.39 0.04 1.57 : 0.90 0.04 1.57\n",
      "mean norm:  0.7290999889373779 0.07216115295886993\n",
      "epoch: 0, loss: 2.3027081883573137, accruacy: 0.2695\n",
      "1.55 1.52 1.57 : 0.91 0.04 1.57\n",
      "mean norm:  0.8540456891059875 0.08449258655309677\n",
      "epoch: 1, loss: 2.039130000210295, accruacy: 0.3043\n",
      "1.55 1.54 1.57 : 0.91 0.04 1.57\n",
      "mean norm:  0.931410014629364 0.09212047606706619\n",
      "epoch: 2, loss: 1.9808444433528196, accruacy: 0.3276\n",
      "1.56 1.55 1.57 : 0.91 0.04 1.57\n",
      "mean norm:  0.9727274775505066 0.09618840366601944\n",
      "epoch: 3, loss: 1.9010257734667382, accruacy: 0.3421\n",
      "1.56 1.55 1.57 : 0.91 0.04 1.57\n",
      "mean norm:  0.9931061863899231 0.09818591922521591\n",
      "epoch: 4, loss: 1.975194415642149, accruacy: 0.3525\n"
     ]
    }
   ],
   "source": [
    "# SO\n",
    "model = TestModel().cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "for e in range(epochs):\n",
    "    for x, y in train_loader:\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y) + 0.1*SO(model.layer3.weight)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    W = model.layer3.weight\n",
    "    Mean, Min, Max = angle_analysis(W)\n",
    "    TMean, TMin, TMax = angle_analysis(torch.t(W))\n",
    "    print(f'{Mean.item():3.2f} {Min.item():3.2f} {Max.item():3.2f} : {TMean.item():3.2f} {TMin.item():3.2f} {TMax.item():3.2f}')\n",
    "    print('mean norm: ', torch.mean(torch.norm(W, dim=1)).item(), torch.mean(torch.norm(W, dim=0)).item())\n",
    "    test_num = 0\n",
    "    hit_num = 0\n",
    "    for x, y in val_loader:\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "        test_num += len(y)\n",
    "        out = model(x)\n",
    "        pred = out.argmax(dim=1, keepdim=True)\n",
    "        hit_num += pred.eq(y.view_as(pred)).sum().item()\n",
    "    print(f'epoch: {e}, loss: {loss}, accruacy: {hit_num/test_num}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9d8139c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.39 0.05 1.57 : 0.90 0.04 1.57\n",
      "mean norm:  0.7337130308151245 0.07255121320486069\n",
      "epoch: 0, loss: 101.31858130709698, accruacy: 0.2651\n",
      "1.39 0.04 1.57 : 0.90 0.24 1.57\n",
      "mean norm:  0.856711208820343 0.08468716591596603\n",
      "epoch: 1, loss: 101.06299517101633, accruacy: 0.2997\n",
      "1.40 0.04 1.57 : 0.90 0.24 1.57\n",
      "mean norm:  0.9327279925346375 0.0921822190284729\n",
      "epoch: 2, loss: 100.8942430170697, accruacy: 0.3207\n",
      "1.40 0.04 1.57 : 0.91 0.24 1.57\n",
      "mean norm:  0.9732166528701782 0.09616829454898834\n",
      "epoch: 3, loss: 100.87449556708997, accruacy: 0.3375\n",
      "1.41 0.04 1.57 : 0.91 0.24 1.57\n",
      "mean norm:  0.9934465289115906 0.09814741462469101\n",
      "epoch: 4, loss: 100.82878269491732, accruacy: 0.3434\n"
     ]
    }
   ],
   "source": [
    "# TSO\n",
    "model = TestModel().cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "for e in range(epochs):\n",
    "    for x, y in train_loader:\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y) + 0.1*SO(torch.t(model.layer3.weight))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    W = model.layer3.weight\n",
    "    Mean, Min, Max = angle_analysis(W)\n",
    "    TMean, TMin, TMax = angle_analysis(torch.t(W))\n",
    "    print(f'{Mean.item():3.2f} {Min.item():3.2f} {Max.item():3.2f} : {TMean.item():3.2f} {TMin.item():3.2f} {TMax.item():3.2f}')\n",
    "    print('mean norm: ', torch.mean(torch.norm(W, dim=1)).item(), torch.mean(torch.norm(W, dim=0)).item())\n",
    "    test_num = 0\n",
    "    hit_num = 0\n",
    "    for x, y in val_loader:\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "        test_num += len(y)\n",
    "        out = model(x)\n",
    "        pred = out.argmax(dim=1, keepdim=True)\n",
    "        hit_num += pred.eq(y.view_as(pred)).sum().item()\n",
    "    print(f'epoch: {e}, loss: {loss}, accruacy: {hit_num/test_num}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "46d8cea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.40 0.05 1.56 : 0.91 0.04 1.57\n",
      "mean norm:  0.8451782464981079 0.08361684530973434\n",
      "epoch: 0, loss: 101.2794976826328, accruacy: 0.2709\n",
      "1.41 0.04 1.57 : 0.91 0.04 1.57\n",
      "mean norm:  0.9665595889091492 0.0956035703420639\n",
      "epoch: 1, loss: 100.95623395687777, accruacy: 0.3169\n",
      "1.26 0.05 1.57 : 0.91 0.04 1.57\n",
      "mean norm:  0.9974905848503113 0.09864223003387451\n",
      "epoch: 2, loss: 100.88144357974193, accruacy: 0.3349\n",
      "1.26 0.04 1.57 : 0.91 0.04 1.57\n",
      "mean norm:  1.003974437713623 0.09926684945821762\n",
      "epoch: 3, loss: 100.83553403829364, accruacy: 0.3426\n",
      "1.57 1.56 1.57 : 0.91 0.04 1.57\n",
      "mean norm:  1.0052741765975952 0.09937497228384018\n",
      "epoch: 4, loss: 100.82762528798462, accruacy: 0.3531\n"
     ]
    }
   ],
   "source": [
    "# DSO\n",
    "model = TestModel().cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "for e in range(epochs):\n",
    "    for x, y in train_loader:\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y) + 0.1*DSO(model.layer3.weight)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    W = model.layer3.weight\n",
    "    Mean, Min, Max = angle_analysis(W)\n",
    "    TMean, TMin, TMax = angle_analysis(torch.t(W))\n",
    "    print(f'{Mean.item():3.2f} {Min.item():3.2f} {Max.item():3.2f} : {TMean.item():3.2f} {TMin.item():3.2f} {TMax.item():3.2f}')\n",
    "    print('mean norm: ', torch.mean(torch.norm(W, dim=1)).item(), torch.mean(torch.norm(W, dim=0)).item())\n",
    "    test_num = 0\n",
    "    hit_num = 0\n",
    "    for x, y in val_loader:\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "        test_num += len(y)\n",
    "        out = model(x)\n",
    "        pred = out.argmax(dim=1, keepdim=True)\n",
    "        hit_num += pred.eq(y.view_as(pred)).sum().item()\n",
    "    print(f'epoch: {e}, loss: {loss}, accruacy: {hit_num/test_num}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4974aec7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
